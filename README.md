---
tags: 'ML notes'
---

# æ©Ÿå™¨å­¸ç¿’çŸ¥è­˜åº«

é¢è©¦å°å‘çš„æ©Ÿå™¨å­¸ç¿’ç­†è¨˜

[Toc]

![](https://i.imgur.com/3FRxtwt.jpg)
[Img source](https://towardsdatascience.com/types-of-machine-learning-interviews-and-how-to-ace-them-51587a95f847)

## GPU
### ç‚ºä»€éº¼ GPU å¯ä»¥åŠ é€ŸçŸ©é™£é‹ç®—?

[How do GPUs speed up Neural Network training?](https://www.youtube.com/watch?v=EKD1kEMNeeU)

1. GPU çš„è¨˜æ†¶é«”é »å¯¬æ›´å¯¬ï¼Œä¸€æ¬¡å¯ä»¥è™•ç†æ›´å¤šè³‡æ–™
2. GPU æä¾›å¹³è¡ŒåŒ– (Parallelization) è™•ç†
3. é›–ç„¶ GPU çš„ cache å’Œ register æ›´å°ï¼Œä½†å»æ›´å¤šã€æ›´å¿«ï¼Œæœ‰è¨±å¤šçš„ streamlined processors (SM)

ä»¥çŸ©é™£é‹ç®—çš„è™•ç†ä¾†èªª

1. ä¸€æ¬¡è™•ç†ä¸€å€‹element -> ç„¡æ³•åˆ©ç”¨åˆ°ä»»ä½•ä¸€å€‹å„ªå‹¢
![](https://i.imgur.com/pa4nqEJ.jpg)
2. åˆ©ç”¨GPU Memory ä¾†è™•ç†element -> å¯åˆ©ç”¨åˆ°å¹³è¡ŒåŒ–çš„å„ªå‹¢
![](https://i.imgur.com/XgOJhFM.jpg)
3. å°‡çŸ©é™£é‹ç®—æ‹†æˆå¹¾å€‹ Blockï¼Œä»¥ Block ç‚ºå–®ä½é€²è¡Œé‹ç®— -> å¯åˆ©ç”¨åˆ°æ‰€æœ‰å„ªå‹¢
![](https://i.imgur.com/24WW4NN.jpg)
![](https://i.imgur.com/poRKBQM.gif)

è€Œ Block multiplication çš„ Block size è·Ÿ SM çš„å€‹æ•¸ä»¥åŠå¦‚ä½•å¯¦ä½œé€™äº›çš„ç´°ç¯€æœƒç”± CUDA å¹«å¿™è™•ç†


## Models

### Linear Regression
- æ‰¾æ®˜å·®æœ€å°çš„ best fit of line (ç”¨ Least Squares Method ç®—å‡ºæ¨¡å‹ä¿‚æ•¸)
- è‹¥çµæœå†åŠ ä¸Š Sigmoid å°±æœƒè®Šæˆ Logistic regression

#### Assuption :
1. Linearity: The relationship between X and the mean of Y is linear.
2. Homoscedasticity: The variance of the residual is the same for any value of X.
3. Independence: Observations are independent of each other.
4. Normality: For any fixed value of X, Y is normally distributed.

#### Test for Significance

- Simple regression
  - t-test, F-test çš†å¯ï¼Œçµæœæœƒä¸€æ¨£
- multiple regression
  - t-test, F-test çš„çµæœæœƒä¸ä¸€æ¨£
    - t-test : test for individual significance (æ¸¬è©¦å–®ä¸€è®Šæ•¸)
      - å¯èƒ½æœƒå› ç‚ºé‡ä¸Šå¤šé‡å…±ç·šæ€§å•é¡Œ (Multicollinearity) è€Œç„¡æ³•æº–ç¢ºä½¿ç”¨
    - F-test : test for overall significance (æ¸¬è©¦æ‰€æœ‰è®Šæ•¸)


#### Coefficient of Determination 

- åˆ©ç”¨ $r^2 = SSR / SST$ (Coefficient of Determination )ä¾†è©•ä¼°
  - Adjusted $r^2$ å°‡è®Šæ•¸çš„æ•¸é‡ä¹Ÿç´å…¥è€ƒé‡ï¼Œè®“æ¨¡å‹ä¸æœƒå› ç‚ºæ²’ç”¨çš„è®Šæ•¸è¶Šä¾†è¶Šå¤šè€Œè¶Šä¾†è¶Šå¥½ (æœƒå°è‡´é›£ä»¥ä¼°è¨ˆä¿‚æ•¸) 
  - $r^2_{adj} = 1 - (1-r^2)(\frac{n-1}{n-p-1})$

#### Error Term ğœ€ çš„å‡è¨­
[Residuals Analysis ](https://www.theopeneducator.com/doe/Regression/residuals-analysis)
1. **normally distributed with zero mean** : The error ğœ€ is a normally distributed random variable with mean of zero
2. **constant (homogeneous) variance** : The variance of ğœ€, denoted by Ïƒ2, is the same for all values of the independent variable
3. **uncorrelated** : The values of ğœ€ are independent

#### Interval
- Prediction interval : å°å–®ä¸€å€‹ given x æ‰€é æ¸¬å‡ºçš„é‚£å–®ä¸€å€‹æ–° y æœƒæœ‰çš„å€é–“ä¼°è¨ˆ
- Confidence interval : å°å–®ä¸€å€‹ given x æ‰€å¯èƒ½é æ¸¬å‡ºçš„é‚£ç¾¤ y çš„å¹³å‡å€¼å€é–“ä¼°è¨ˆ
- Prediction interval çš„ margin of error æœƒæ¯”è¼ƒå¤§ (å€é–“è¼ƒå¤§)

#### Residual Analysis : ç¢ºä¿ Error Term å‡è¨­æ˜¯å°çš„
- ç”¨ä¾†ä¼°è¨ˆè§€å¯Ÿæˆ–é æ¸¬åˆ°çš„èª¤å·®(residuals)èˆ‡éš¨æ©Ÿèª¤å·®(stochastic error)æ˜¯å¦ä¸€è‡´
  - å‡å¦‚æ²’æœ‰ä¸€è‡´çš„è©±é‚£éº¼ Interval å’Œ Significance éƒ½æœƒå‡ºéŒ¯
- æ­£å¸¸æ¨¡å‹çš„æ“¬åˆï¼Œæ®˜å·®æ‡‰è©²ä»¥0ç‚ºä¸­å¿ƒä¸¦å¹³å‡æ•£ä½ˆåœ¨è¢«æ“¬åˆå€¼é»é™„è¿‘ï¼Œè€Œä¸”æ˜¯ä»¥å°ç¨±çš„å½¢å¼å‘ˆç¾
  - æ­£å¸¸çš„æ®˜å·®åœ–è¦è¡¨ç¾å‡ºéš¨æ©Ÿæ€§(random)å’Œä¸å¯é æ¸¬æ€§(unpredictable)ï¼Œæ®˜å·®ä¸æ‡‰è©²åŒ…å«ä»»ä½•å¯é æ¸¬çš„è³‡è¨Š
- Simple regression ä¸­é‡å° $\hat{y}$ å’Œ $\hat{x}$ çš„æ®˜å·®åœ–çµæœä¸€æ¨£
- Multiple regression ä¸­å‰‡æ˜¯åªæœƒé‡å° $\hat{y}$ åšæ®˜å·®åœ–ä¾†ç¢ºèªæ¨¡å‹å‡è¨­æ˜¯å¦ç¬¦åˆ

![](https://i.imgur.com/O1vi3k5.jpg)
![](https://i.imgur.com/fL2pDVm.jpg)

#### Standardization Residual Plot
1. ç”¨ä¾†åˆ¤æ–· error term æ˜¯å¦ç‚ºå¸¸æ…‹åˆ†å¸ƒ
2. ç”¨ä¾†åˆ¤æ–· outlier : è‹¥ standardization residual > +2 æˆ–æ˜¯ < -2 å°±ä»£è¡¨é€™å€‹è³‡æ–™é»æ˜¯ outlier
   - ç•¶é‡ä¸Šç•°å¸¸å¤§çš„ outlier çš„æ™‚å€™æœƒå¿…é ˆæ”¹ç”¨**å­¸ç”ŸåŒ–æ®˜å·®(studentized deleted residuals, åˆç¨±tåŒ–æ®˜å·®)**ï¼Œä¸éé€šå¸¸å°±ç›´æ¥ç”¨å­¸ç”ŸåŒ–æ®˜å·®ä¾†åˆ¤æ–·äº†
      - studentized deleted residual çš„åšæ³•æ˜¯æŠŠå¾…æ¸¬é»åˆªé™¤æ‰å¾Œè©•ä¼°å¾…æ¸¬é»èˆ‡æ ¹æ“šè©²ä¼°è¨ˆæ¨¡å‹é æ¸¬çµæœä¹‹é›¢å·®
      -  è‹¥ studentized deleted residual çµ•å°å€¼åé«˜ï¼Œè¡¨ç¤ºé€™å€‹è³‡æ–™é»èˆ‡å…¶ä»–é»å¯èƒ½å±¬ä¸åŒæ¨¡å‹ï¼Œå³é€™ä¸€é»æ˜¯outlier
      - Studentized deleted residual 
        ![](https://i.imgur.com/mt1Bezv.jpg)


#### Outlier vs Leverage vs influential Points
[Outlier, Leverage, and Influential Points](https://www.theopeneducator.com/doe/Regression/outlier-leverage-influential-points)
- **Outlier point** æ˜¯ç›¸å°æ–¼ x æˆ–æ˜¯ y çš„ç•°å¸¸è³‡æ–™é» (é€šå¸¸æ˜¯y-outliersæ¯”è¼ƒå¸¸è¦‹)
  - y-outlier : é€šå¸¸æ®˜å·®ç›¸è¼ƒæ–¼å…¶ä»–è³‡æ–™é»ç‰¹åˆ¥é«˜çš„æœƒæ˜¯ y-outlier ($|Studentized Residual| > 2$)
  - x-outlier : é€šå¸¸ diagonal element of the hat matrix, HI å€¼å¤§æ–¼ 2p/n çš„æœƒæ˜¯ x-outlier 
    - p æ˜¯è®Šæ•¸çš„æ•¸é‡ï¼Œè‹¥ç‚ºå–®è®Šæ•¸è¿´æ­¸å°±æ˜¯åªæœ‰ x å’Œ yï¼Œä¹Ÿå°±æ˜¯ p=2
    - n æ˜¯ observation çš„æ•¸é‡ 
- **Influential point** æŒ‡çš„æ˜¯ä¸€å€‹è¶³ä»¥å½±éŸ¿æ•´æ¢è¿´æ­¸ç·šçš„è³‡æ–™é»
  - åˆ©ç”¨ DFFITS (Studentized Difference in fits) æˆ–æ˜¯ COOK distance å¯ä»¥æ‰¾å‡º
    - å…¶ä¸­ DFFITS ä»£è¡¨çš„æ˜¯ç§»é™¤ç¬¬ $i_{th}$ è³‡æ–™é»ä¹‹å¾Œï¼Œå°å–®ä¸€å€‹ fitted value çš„å½±éŸ¿
    - Cook's distance ä»£è¡¨çš„æ˜¯ç§»é™¤ç¬¬ $i_{th}$ è³‡æ–™é»ä¹‹å¾Œï¼Œå°æ‰€æœ‰ fitted value çš„å½±éŸ¿  
  ![](https://i.imgur.com/0rlh1v2.jpg)
  - è‹¥ DFFITS çš„çµ•å°å€¼åœ¨é‡ç´šç‚ºå°~ä¸­ç­‰çš„è³‡æ–™ç´šä¸‹è¶…é1å°±è¢«åˆ¤æ–·ç‚º influentialï¼Œåœ¨å¤§è¦æ¨¡çš„è³‡æ–™é›†å‰‡æ˜¯ä»¥ $|DFFITS|>2\sqrt{\frac{p}{n}}$ä¾†åˆ¤æ–·
  - è‹¥ COOK distance éå¸¸å¤§ä¹Ÿæœƒè¢«åˆ¤æ–·ç‚º influential (æˆ–æ˜¯ä»¥ $D_i > \frac{4}{n}$ã€å°‡$D_i$ä»£å…¥F(p, n-p)æ‰€ç”¢ç”Ÿçš„æ©Ÿç‡å€¼æ˜¯å¦å¤§æ–¼50%ä¾†åˆ¤æ–·)
    - å¯ä»¥æŠŠ COOK distance å¸¶å…¥ Fåˆ†é… (1-FDIST(COOK distance, p , è‡ªç”±åº¦)ä¹‹ä¸­ä¾†åˆ¤æ–·é€™å€‹è³‡æ–™é»åˆ°åº•æœ‰å¤š influentialï¼Œç®—å‡ºä¾†çš„æ©Ÿç‡è¶Šé«˜ä»£è¡¨è¶Š influenital    
![](https://i.imgur.com/Xvw62TM.jpg =200x100)

- **Leverage point** æŒ‡çš„æ˜¯ä¸€å€‹ xå€¼éå¸¸ç•°å¸¸ä½†å…¶ y åˆ follow predicted regression line
  - é€™ç¨®è³‡æ–™é»æœƒä½¿å¾— p-value è®Šæ›´å°ï¼Œä¹Ÿæœƒä½¿å¾—r-square è®Šæ›´å¤§ä½†å»ä¸æœƒå½±éŸ¿åˆ°æ¨¡å‹ä¿‚æ•¸ (å¯èƒ½æœƒå°è‡´é«˜ä¼°æ¨¡å‹)
  - diagonal element of the hat matrix, HI å¯ä»¥ç”¨ä¾†æ‰¾å‡º Leverage point
![](https://i.imgur.com/3OZFCvm.jpg)
![](https://i.imgur.com/dJSsErG.jpg)
![](https://i.imgur.com/LgJTETK.jpg)

### Logistic Regression
- åŠ ä¸Š Sigmoid çš„ Linear Regression
  - ç‚ºç”šéº¼æ˜¯ Sigmoid?
    - å€¼åŸŸ [0,1] èˆ‡æ©Ÿç‡ç›¸ç¬¦
    - è¶Šé«˜è¶Šå¹³å¦ (è¶Šä¾†è¶Šé›£ä¸Šå‡) ä»¥åŠè¶Šä½è¶Šå¹³å¦ (è¶Šä¾†è¶Šé›£ä¸‹é™) çš„ Så‹å‡½æ•¸ (ä¹™ç‹€å‡½æ•¸) ç‰¹æ€§
![](https://i.imgur.com/IKG2yXq.jpg)
- ä½¿ç”¨ Maximum Likelihood Estimation ä¾†ä¼°è¨ˆæ¨¡å‹åƒæ•¸ï¼Œè€Œéæœ€å°å¹³æ–¹æ³•


#### Assuptions : [Explained](https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290)
1. **Appropriate Outcome Type :** è¼¸å‡ºæ‡‰è©²æ˜¯ Binaryï¼Œä¸ç„¶å°±å¾—ä½¿ç”¨  multinomial or ordinal logistic regression 
2. **The Observations are Independent :** è³‡æ–™ä¹‹é–“ä¸äº’ç›¸å½±éŸ¿ 
3. **There is No Multicollinearity Among Explanatory Variables :**  è®Šæ•¸ä¹‹é–“æ²’æœ‰å…±ç·šæ€§é—œä¿‚ï¼Œå¯ç”¨ Variance Inflation Factor (VIF) ä¾†æ¸¬ï¼Œè‹¥ VIF > 5 æˆ– 10 å¯èƒ½å°±æœ‰å…±ç·šæ€§
4. **There are No Extreme Outliers :** å¯ç”¨ Cook's distance ä¾†æ¸¬ï¼Œçœ‹æ˜¯å¦ > 4/N
5. **There is a Linear Relationship Between Explanatory Variables and the Logit of the Response Variable** : è®Šæ•¸å’Œè¼¸å‡ºçš„æ©Ÿç‡(logit)ä¹‹é–“æœ‰ç·šæ€§é—œä¿‚ï¼Œå¯ç”¨Box-Tidwell Test æˆ–æ˜¯ç•«åœ–æ¸¬
6. **The Sample Size is Sufficiently Large :** å‡ºç¾é »ç‡æœ€ä½çš„è®Šæ•¸è‡³å°‘è¦æœ‰10ç­†è³‡æ–™ï¼Œä¹Ÿå°±æ˜¯èªªéœ€è¦çš„æœ€å°è³‡æ–™é‡æœƒ = `10*è®Šæ•¸æ•¸é‡ / æœ€ä½çš„æœŸæœ›æ©Ÿç‡`
   - if you have 3 explanatory variables and the expected probability of the least frequent outcome is 0.20, then you should have a sample size of at least (10*3) / 0.20 = 150

#### Comparison with Linear Regression

Differences
- Logistic regression ä¸éœ€è¦ x å’Œ y æœ‰ç·šæ€§é—œä¿‚ï¼Œä½†ä»éœ€è¦ x å’Œé æ¸¬çµæœçš„ log-oddsHæœ‰ç·šæ€§é—œä¿‚
- Logistic regression ä¸éœ€è¦ Homoscedasticity 
- error terms (residuals) ä¸éœ€è¦æ˜¯å¸¸æ…‹åˆ†å¸ƒ

Similarities
- ç„¡æ³•è™•ç† multicollinearity
- Observations are independent of each other

#### Metrics

- Cross Validation
  - ROC/AUCã€Confusion Matrix (Precision/Recall)
- Concordance : æ‰€æœ‰é æ¸¬çµæœ(0,1)æˆå°çš„æ©Ÿç‡å€¼ä¸­ï¼ŒçœŸå¯¦ç‚º1çš„äº‹ä»¶ï¼Œ1çš„æ©Ÿç‡é«˜æ–¼0çš„æ©Ÿç‡ä½”æ‰€æœ‰æˆå°è³‡æ–™çš„æ¯”ä¾‹ã€‚
  - ç†æƒ³ä¸­ï¼Œè©²æ¯”ä¾‹è¶Šé«˜è¶Šå¥½ï¼Œå³è¡¨ç¤ºæ‰€æœ‰é æ¸¬(0,1)çš„æ©Ÿç‡å€¼ï¼Œè‹¥çœŸå¯¦ç‚º1ï¼Œå‰‡1çš„é æ¸¬æ©Ÿç‡ç†æ‡‰éƒ½å¤§æ–¼0çš„é æ¸¬æ©Ÿç‡ã€‚ï¼ˆdiscordanceå‰‡ç‚ºç›¸åçµæœçš„æ¯”ä¾‹ï¼Œtiedå‰‡ç‚ºæ©Ÿç‡ç„¡å·®åˆ¥çš„çµæœæ¯”ä¾‹ï¼Œä¸‰æ¯”ä¾‹ç›¸åŠ æ‡‰ç‚º100%ï¼‰ã€‚
- Others [img source](https://taweihuang.hpd.io/2017/12/22/logreg101/)
![](https://i.imgur.com/3LrCw01.jpg)

### Ensemble : Bagging, Boosting, Stacking

- å–®ä¸€å€‹æ¨¡å‹éƒ½ç¨±ç‚º Weak learner
- å¤šå€‹æ¨¡å‹çµ„åˆåœ¨ä¸€èµ·å°±ç¨±ç‚º Strong learner
- çµ„åˆçš„æ–¹å¼å¯åˆ†ç‚º homogeneous weak learners è·Ÿ  heterogeneous weak learners
  - heterogeneous : stacking 
  - homogeneous : bagging and boosting

#### Bagging : Reducing Variance
å¾è¨“ç·´è³‡æ–™ä¸­éš¨æ©ŸæŠ½å–(å–å¾Œæ”¾å›)æ¨£æœ¬è¨“ç·´å¤šå€‹åˆ†é¡å™¨(è¦å¤šå°‘å€‹åˆ†é¡å™¨è‡ªå·±è¨­å®š)ï¼Œæ¯å€‹åˆ†é¡å™¨çš„æ¬Šé‡ä¸€è‡´æœ€å¾Œç”¨æŠ•ç¥¨æ–¹å¼(Majority vote)å¾—åˆ°æœ€çµ‚çµæœï¼Œè€Œé€™ç¨®æŠ½æ¨£çš„æ–¹æ³•åœ¨çµ±è¨ˆä¸Šç¨±ç‚ºbootstrap

- åˆç¨± Bootstrap aggregation
- å„ªé» : åŸå§‹è¨“ç·´æ¨£æœ¬ä¸­è‹¥æœ‰ noise data æ™‚ï¼Œé€é Bagging çš„æŠ½æ¨£å°±æœ‰æ©Ÿæœƒä¸è®“ noise dataè¢«è¨“ç·´åˆ°ï¼Œæ‰€ä»¥å¯ä»¥é™ä½æ¨¡å‹çš„ä¸ç©©å®šæ€§å’Œoverfittingçš„å¯èƒ½
- Example : Random forestã€KNN
  - Deep decision tree å°±æ˜¯ä¸€ç¨® High variance ä½† low bias çš„æ¨¡å‹ï¼Œå› æ­¤é©åˆä½¿ç”¨ Bagging

Boostrapping : å–å¾Œæ”¾å›çš„éš¨æ©ŸæŠ½æ¨£æ–¹æ³•
![](https://i.imgur.com/Rs2Danb.png)

[Workflow of Bagging](https://medium.com/ml-research-lab/bagging-ensemble-meta-algorithm-for-reducing-variance-c98fffa5489f)

![](https://i.imgur.com/3o5U36s.png)

#### Boosting : Reducing Bias

Boosting æ˜¯å°‡å¾ˆå¤šå€‹ weak classifier å…ˆç”¨ç›¸åŒæ¬Šé‡åˆå§‹åŒ–ï¼Œé¸å‡ºå…¶ä¸­ error å¤§çš„è¾¨è­˜è³‡æ–™ (é›£ä»¥è¾¨è­˜çš„è³‡æ–™)ï¼Œä¸¦é¸å‡ºèƒ½å°é€™äº›é›£ä»¥è¾¨è­˜çš„è³‡æ–™é€²è¡Œè¾¨è­˜çš„æ¨¡å‹é€²è¡Œåˆæˆï¼Œè®Šæˆä¸€å€‹ Strong classifier

- Boosting çš„ Model å’Œ data éƒ½æ˜¯æœ‰æ¬Šé‡çš„ï¼Œè€Œ Bagging çš„ Model æ¬Šé‡éƒ½ä¸€æ¨£ä¸”è³‡æ–™æ˜¯éš¨æ©ŸæŠ½æ¨£
- å„ªé» : åœ¨è¨“ç·´è³‡æ–™å¤ªå°‘çš„æ™‚å€™å¯ä»¥é¿å… under fittingï¼Œä¹Ÿå¯ä»¥é¿å…å› ç‚ºé©—è­‰è³‡æ–™å¤ªå°‘è€Œ over fitting
- ç¼ºé»æ˜¯ç„¡æ³• done in parallel (unlike bagging)ï¼Œä½†å…¶è¡ä¼¸æ¨¡å‹æ”¹å–„äº†é€™å€‹å•é¡Œ
- Example : Gradient Boosting, Adaboost, XGBoost
  - Shallow decision tree å°±æ˜¯ä¸€ç¨® High Bias ä½† low variance çš„æ¨¡å‹

[Workflow of Boosting](https://medium.com/ml-research-lab/boosting-ensemble-meta-algorithm-for-reducing-bias-5b8bfdce281)

![](https://i.imgur.com/CQt81iv.png)

#### Stacking : improve predictions
stacking çš„æ–¹æ³•å°±æ˜¯è¨“ç·´å„ç¨®ä¸åŒçš„ model, ç„¶å¾Œæœƒæœ‰ç¬¬äºŒå±¤å»ç¶œåˆå‰é¢ model çš„è§€é»å¾—åˆ°ä¸€å€‹æ–°çš„çµæœ

[Workflow of stacking](https://medium.com/ml-research-lab/stacking-ensemble-meta-algorithms-for-improve-predictions-f4b4cf3b9237)

![](https://i.imgur.com/fpR4BMW.png)

å¯åˆ†ç‚º
1. Algorithm Stacking
   - ä»¥ä¸åŒçš„modelè¨“ç·´å¾Œä½œç‚ºfeature, ç•¶ä½œç¬¬äºŒå±¤modelçš„featureè·ŸåŸæœ¬çš„featureåƒé›œåœ¨ä¸€èµ·. åœ¨é€™æ¨£çš„æ€ç¶­ä¸‹å°±æ˜¯ç”¨ä¸åŒäººçš„è§€é»ç¶œåˆä¹‹å¾Œå¯ä»¥å¾—åˆ°ä¸€å€‹æ¯”è¼ƒæ²’æœ‰åè¦‹çš„ç­”æ¡ˆæ‰€å¾—å‡ºçš„çµæœ
2. Features Stacking
   - åˆ‡åˆ†ä¸åŒçš„feature subsetè¨“ç·´å¾Œç•¶ä½œfeature, ç•¶ä½œä¹‹å¾Œmodel learningçš„featureåšä¸åŒçš„weak learnerï¼Œä¹‹å¾Œå†çµ„åˆèµ·ä¾†ä¸€èµ·å­¸ç¿’
3. Dataset Stacking
   - åˆ‡åˆ†ä¸åŒçš„datasetè¨“ç·´å¾Œç•¶ä½œä¸åŒå±¤çš„training set
   - é€™å€‹æ€ç¶­è·Ÿ bagging ä¸ä¸€æ¨£çš„é»åœ¨æ–¼ bagging æƒ³åšåˆ°çš„äº‹æƒ…æ˜¯ç”¨åŒæ¨£çš„ classifier å»è¨“ç·´ä¸åŒå­é›†åˆçš„ dataset, æƒ³é¿å…çš„äº‹æƒ…æ˜¯æŸå€‹æ¼”ç®—æ³•éåº¦optimizeç•¶å‰è¨“ç·´é›†çš„çµæœ
   - è€Œåœ¨stackingè£¡é¢çš„åšæ³•å…¶å¯¦æ˜¯åœ¨ç¬¬ä¸€å±¤çš„weak learnerçš„æ™‚å€™éƒ½ç”¨åŒä¸€å€‹subset,å¾—åˆ°çš„çµæœå»ç”¨åœ¨ç¬¬äºŒå±¤çš„modelè£¡é¢ç•¶ä½œfeature predictor, å†ç”¨å¦ä¸€å€‹subsetä¾†åšç¬¬äºŒå±¤leanerçš„è¨“ç·´é›†ã€‚

**å°‡å„ç¨®ä¸åŒçš„æ¨¡å‹ stacking èµ·ä¾†çš„æ–¹æ³• :**
1. è¨“ç·´å¦ä¸€å€‹æ¨¡å‹ä¾†ç¶œåˆï¼Œæ¯”å¦‚ç·šæ€§ã€NN
2. Averaging
![](https://i.imgur.com/tQFUAXu.png)
4. Weighted Averaging
![](https://i.imgur.com/7VbWeor.png)

### CNNs

[CS231n](https://cs231n.github.io/convolutional-networks/)
[ML Lecture 10: Convolutional Neural Network](https://www.youtube.com/watch?v=FrKWiRv254g)
#### Indutive bias of CNN: 
1.  Locality : Pixels near one another are related ç›¸é€£çš„pixelæ˜¯æœ‰é—œé€£çš„
2.  weight sharing : Different portions of an image should be processed identically regardless of their absolute location ä¸€å¼µåœ–ç‰‡ä¸­çš„ä¸åŒå€åŸŸéƒ½æœƒåšä¸€æ¨£çš„è™•ç† (åŒä¸€å€‹ Filter åœ¨ä¸åŒçš„åœ–ç‰‡å€åŸŸä¸Šçš„ weight éƒ½æ˜¯ä¸€æ¨£çš„)
    
#### ç‚ºç”šéº¼è¦åš Pooling?

æ–¹æ³•é€šå¸¸æœ‰ Max Pooling, Mean Pooling, Stochastic-Pooling

1. æ¸›å°‘å¾ŒçºŒlayeréœ€è¦åƒæ•¸ï¼ŒåŠ å¿«ç³»çµ±é‹ä½œçš„æ•ˆç‡
2. å…·æœ‰æŠ—å¹²æ“¾çš„ä½œç”¨ï¼šåœ–åƒä¸­æŸäº›åƒç´ åœ¨é„°è¿‘å€åŸŸæœ‰å¾®å°åç§»æˆ–å·®ç•°æ™‚ï¼Œå°Pooling layerçš„è¼¸å‡ºå½±éŸ¿ä¸å¤§ï¼Œçµæœä»æ˜¯ä¸è®Šçš„ (translation invarianceã€rotation invarianceã€scale invariance)
   - æ¯”å¦‚æ‰‹å¯«è¾¨è­˜åœ–ç‰‡çš„æ•¸å­—å¯èƒ½æœƒæœ‰äº›å¾®çš„ä¸åŒï¼Œåœ–ä¸€çš„æ•¸å­—åå·¦ï¼Œåœ–äºŒçš„æ•¸å­—åå³ä¸€é»ï¼Œæ­¤æ™‚åœ–ä¸€åˆ°åœ–äºŒåªå·®ä¸€å€‹å–®ä½ï¼Œé‚£éº¼å°±ç®—æŠŠåœ–ç‰‡å¾16x16åšPoolingè®Šæˆ8x8ä¹Ÿæœƒå¾—åˆ°ä¸€æ¨£çš„ç‰¹å¾µå’Œä½ç½® (ä¹Ÿå°±æ˜¯ç›¸åŒé€™ç¨®æƒ…æ³çš„åœ–ç‰‡éƒ½æœƒå¾—åˆ°é€™æ¨£çš„çµæœ)
   - Translational invariance (å¹³ç§»ä¸è®Šæ€§)
![](https://i.imgur.com/iMiB1W1.jpg)
   - Rotation invariance (æ—‹è½‰ä¸è®Šæ€§)
![](https://i.imgur.com/RLX7CIb.jpg)
   - Scale Invariance (å°ºåº¦ä¸è®Šæ€§)
![](https://i.imgur.com/VFONFf5.jpg)


#### è·Ÿ Fully-connected layer çš„å·®åˆ¥ ?
[CNN vs MLP in image](https://medium.com/analytics-vidhya/cnn-convolutional-neural-network-8d0a292b4498)
- CNN å…¶å¯¦å°±æ˜¯ FC layer æ‹¿æ‰ä¸€äº›layerçš„çµæœ
  - CNN æ˜¯åªå°è·Ÿ filter åŒæ¨£å¤§å°çš„å€åŸŸ(input)åš Fully connected ç®— inner product
  - è€Œ FC layer æ˜¯å°æ–¼æ•´å¼µåœ–ç‰‡åš Fully connected ç®— inner product

#### åœ–ç‰‡çš„è¼¸å‡ºå¤§å°

å…¬å¼:  $(Wâˆ’F+2P)/S+1$
- $W$ : Input image size
- $F$ : Filter size
- $P$ : Amount of zero-padding
- $S$ : Stride size
Example : 
- Input 7x7 image and 3x3 Filter with Stride 1 and pad 0
- Will ouput $(7-3+2*0)/1 + 1 =$ 5x5 output image
  - è¨ˆç®—ä¸Šéœ€è¦æ³¨æ„è‹¥ç„¡æ³•ç®—å‡ºæ•´æ•¸çš„ output image size å°±ç„¡æ³•ä½¿ç”¨é€™å€‹åƒæ•¸ !

Convolutionæœ‰å¤šå°‘å€‹Filterï¼ŒOutputå°±æœƒæœ‰å¤šå°‘å€‹Filter (å³ä½¿ç–Šäº†å…©å€‹Convolutionä¹Ÿæ˜¯ä¸€æ¨£ï¼Œåªæ˜¯Filterçš„é«˜æœƒè®Š)
- ç¬¬ä¸€å€‹Conv filterä¸­æœ‰9å€‹åƒæ•¸
- ç¬¬äºŒå€‹Conv filterä¸­æœ‰25*9=225å€‹åƒæ•¸
![](https://i.imgur.com/v2z3WDr.jpg)

### RNNs

#### Indutive bias of RNN:
å‡è¨­è³‡æ–™åœ¨æ™‚åºä¸Šæ˜¯æœ‰é—œä¿‚çš„ï¼Œæ­·å²è³‡æ–™æœƒå½±éŸ¿æœªä¾†è³‡æ–™

### Attentions

#### ViT vs CNN ç›¸é—œç ”ç©¶
[Differences of ViT and CNNs representation](https://medium.com/syncedreview/google-brain-uncovers-representation-structure-differences-between-cnns-and-vision-transformers-83b6835dbbac)


### GNN

[A Gentle Introduction to Graph Neural Networks](https://distill.pub/2021/gnn-intro/)


### æ¨¡å‹å£“ç¸®

#### Parameter pruning

- å¯åˆ†ç‚º one-shot å’Œ iteration pruning
  - **One-shot** : Train -> è©•ä¼°ç¥ç¶“å…ƒã€layerã€kernel çš„é‡è¦æ€§ -> å»æ‰æœ€ä¸é‡è¦çš„ -> Fine-tuning -> åœæ­¢ pruning
    - åˆç¨±éœæ…‹å‰ªæ 
  - **Iteration** : Train -> è©•ä¼°ç¥ç¶“å…ƒã€layerã€kernel çš„é‡è¦æ€§ -> å»æ‰æœ€ä¸é‡è¦çš„ -> Fine-tuning -> åˆ¤æ–·æ˜¯å¦ç¹¼çºŒ pruning -> è‹¥ä¸éœ€å‰‡åœæ­¢ pruning
    - åˆç¨±å‹•æ…‹å‰ªæ 
- ä¹Ÿåˆ†ç‚º Structured å’Œ unstructured pruning
  - **Structured** : ç›´æ¥å»æ‰æ•´å€‹ channelã€kernelã€layer çš„çµæ§‹åŒ–è³‡è¨Š (ç›´æ¥åˆªæ‰ä¸€å€‹layerä¹‹é¡çš„)
     - é›–ç„¶å£“ç¸®ç‡ä½ä½†é©åˆåœ¨ç¡¬é«”ä¸Šé‹ä½œ 
  - **Unstructured** : è€ƒæ…®æ¯å€‹ kernel ä¸­çš„æ¯å€‹elementï¼Œåˆªé™¤å…¶ä¸­ä¸é‡è¦çš„åƒæ•¸
    - ä¹Ÿç¨±ç‚º sparse pruningï¼Œå› ç‚ºå…¶å¾—åˆ°çš„æ¬Šé‡çŸ©é™£æœƒæ˜¯ç¨€ç–çš„ï¼Œéœ€è¦ä¸€äº›ç¡¬é«”å»ç‰¹åˆ¥åŠ é€Ÿ
    - é›–ç„¶å£“ç¸®ç‡é«˜ä½†ä¸å¤ªé©åˆåœ¨ç¡¬é«”ä¸Šé‹ä½œ
![](https://i.imgur.com/psHYeKf.jpg)


Parameter pruning åŸºæœ¬æµç¨‹
![](https://i.imgur.com/WsobBlU.jpg)
- æ˜¯å¦è¦ Pruning æ˜¯åŸºæ–¼å‰é¢é‚£å€‹é è¨“ç·´æ‰€å¾—åˆ°çš„æ¬Šé‡å»åˆ¤æ–·çš„
- å…¶ä¸­ Fine-truning æ˜¯æŒ‡è¦å¾®èª¿æ¨¡å‹ä»¥é‡æ–°ç²å¾—å› ç‚ºä¸Ÿå¤±åƒæ•¸è€Œå¤±å»çš„èƒ½åŠ›
  - é€šå¸¸æœƒå°‡ prune ä¹‹å¾Œçš„ç¶²è·¯ä¸­è¢«ä¿ç•™ä¸‹ä¾†çš„åƒæ•¸(è¢«ä¿ç•™ä¸‹ä¾†çš„åƒæ•¸å°±æ˜¯é‡è¦åƒæ•¸)ç”¨ä¾†åˆå§‹åŒ–é€™å€‹ä¿®å‰ªå¾Œçš„ç¶²è·¯ï¼Œå†é‡æ–°åœ¨è¨“ç·´è³‡æ–™é›†ä¸Šè¨“ç·´

çŸ›ç›¾çš„é»? [Rethinking the Value of Network Pruning](https://arxiv.org/abs/1810.05270)
   1. æ¨¡å‹éåº¦åƒæ•¸åŒ–ï¼Œåªç•™ä¸‹è¨“ç·´è³‡æ–™é›†ä¸Šæœ‰æ•ˆçš„åƒæ•¸ä¸ä»£è¡¨æ”¾åˆ°ç¾å¯¦å°±æœƒæœ‰æ•ˆ
   2. å¤§æ¨¡å‹çš„é‡è¦æ¬Šé‡ä¸ä¸€å®šæœ‰åŠ©æ–¼ä¿®å‰ªå¾Œçš„å°æ¨¡å‹
   3. ä¿®å‰ªç¶²è·¯æ‡‰è©²ä»¥æ•´é«”æ¶æ§‹ç‚ºä¸»è€Œéåˆ©ç”¨ç¹¼æ‰¿é‡è¦åƒæ•¸ä¾†ä¸»å°å…¶ä¿®å‰ª

#### Lottery Ticket Hypothesis 

- 2019å¹´ç™¼è¡¨çš„ICLR best paper[The Lottery Ticket Hypothesisï¼š Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- æå‡ºåªè¦å°æ¨¡å‹çš„è¨“ç·´æ™‚æ‰€ç”¢ç”Ÿçš„å­ç¶²è·¯é€²è¡Œå¾ˆå¥½çš„é‡æ–°åˆå§‹åŒ–

ä»€éº¼æ˜¯å½©ç¥¨å‡è¨­?
- ä»»ä½•å¯†é›†ã€éš¨æ©Ÿåˆå§‹åŒ–çš„åŒ…å«å­ç¶²è·¯(å½©ç¥¨ç¶²è·¯)çš„å‰é¥‹ç¶²è·¯ ï¼Œç•¶å°‡å…¶éš”é›¢è¨“ç·´æ™‚ï¼Œå¯ä»¥åœ¨ç›¸ä¼¼çš„ç–Šä»£æ¬¡æ•¸å…§é”åˆ°èˆ‡åŸå§‹ç¶²è·¯ç›¸ç•¶çš„ test accuracy

å¦‚ä½•å¾—åˆ°å½©ç¥¨ç¶²è·¯?
åˆ©ç”¨ iteration pruning æ–¹å¼æ‰¾
1. éš¨æ©Ÿåˆå§‹åŒ–ç¥ç¶“ç¶²è·¯
2. è¨“ç·´é€™å€‹ç¶²è·¯ç›´åˆ°æ”¶æ–‚
3. prune æ‰éƒ¨åˆ†æ¬Šé‡
4. å°‡ä¿®å‰ªå¾Œçš„ç¶²è·¯åˆ©ç”¨æ­¥é©Ÿ1çš„æ–¹å¼éš¨æ©Ÿåˆå§‹åŒ–
5. åˆ¤æ–·æ­¥é©Ÿ4çš„ç¶²è·¯æ˜¯å¦ç‚ºå½©ç¥¨ç¶²è·¯ï¼Œè¨“ç·´å­ç¶²è·¯ä¾†æ¯”è¼ƒå…¶ test accuracy

é€™å€‹å½©ç¥¨(å­)ç¶²è·¯æœ‰ä»€éº¼ç‰¹å¾µ?
- åœ¨ç›¸åŒçš„è¿­ä»£æ¬¡æ•¸ä¸‹å¯ä»¥é”åˆ°è·ŸåŸå§‹ç¶²è·¯ä¸€æ¨£çš„çµæœ
- è¨“ç·´ä¸Šæ¯”åŸå§‹ç¶²è·¯æ›´å¿«æ³›åŒ–ä¸” test accuracy æ›´å¿«

**ç¾æ³å°±æ˜¯å¤§å®¶é‚„æ˜¯ä»¥å‚³çµ±æ–¹æ³•å±…å¤šï¼Œä½†æŒçºŒåœ¨ç ”ç©¶å¦‚ä½•æœ‰æ•ˆåœ°åšå‡ºå½©ç¥¨ç¶²è·¯** - 2021/8/4

#### knowledge distillation


## Training Model

Bias-Variance TTradeoff
![](https://i.imgur.com/vkaEmkn.jpg)

### Overfitting

Ovefitting : training error ä½ä½† testing error é«˜

åŸå›  : 
1. æ¨¡å‹éæ–¼è¤‡é›œ (æ¨¡å‹å®¹é‡éé«˜)
2. æ¯ä¸€ç¨®è³‡æ–™éƒ½å¤ªå°‘ï¼Œç„¡æ³•æ‰¾å‡ºçœŸæ­£é€šç”¨çš„ç‰¹å¾µ

å¦‚ä½•é¿å…æˆ–æ˜¯è§£æ±º : 
1. å–å¾—æ›´å¤šæœ‰ç”¨çš„è³‡æ–™
2. æ¸›å°‘å±¤æ•¸ã€åƒæ•¸ä¾†é™ä½æ¨¡å‹è¤‡é›œåº¦
3. Early Stopping
4. åœ¨ä¸æ”¹åƒæ•¸å’Œå±¤æ•¸çš„æƒ…æ³ä¸‹åš Dropout
5. åœ¨ä¸æ”¹åƒæ•¸å’Œå±¤æ•¸çš„æƒ…æ³ä¸‹åš Regularization



### Regularization (æ­£è¦åŒ–)
[Regularization æ–¹æ³• : Weight Decay , Early Stopping and Dropout](https://hackmd.io/@allen108108/Bkp-RGfCE)

[Lasso vs Ridge vs Elastic Net | ML
](https://www.geeksforgeeks.org/lasso-vs-ridge-vs-elastic-net-ml/)

[Regularization in ML](https://towardsdatascience.com/types-of-regularization-in-machine-learning-eb5ce5f9bf50)
#### Weight decay(æ¬Šé‡è¡°æ¸›) : Lasso and Ridge regression
- å°±æ˜¯ L1 / L2 Regularization
- ç”¨ä¾†åˆ¤æ–·æ¨¡å‹çš„è¤‡é›œåº¦ä¸¦åŠ ä»¥æ‡²ç½°
- è¶Šå¤§çš„weightæ‡²ç½°å°±è¶Šå¤§ï¼Œè¶Šå°çš„weightæ‡²ç½°å°±è¶Šå°
    - L1, Lasso ç•¶nå¾ˆå°æ™‚è‡³å¤šåªèƒ½é¸å‡ºnå€‹è®Šé‡ï¼Œè€Œä¸”ç„¡æ³•è™•ç† group selectionï¼Œä¹Ÿå°±æ˜¯èªªç•¶æœ‰ä¸€ç¾¤åƒæ•¸ä¹‹é–“çš„ pairwise correlations å¾ˆå¼·çš„æ™‚å€™ï¼ŒLasso åªæœƒå¾é€™ç¾¤åƒæ•¸ä¸­å–ä¸€å€‹å‡ºä¾† 
    - L2, Ridge æ¸›å°‘æ¨¡å‹è¤‡é›œåº¦ä½†æ²’æœ‰æ¸›å°‘åƒæ•¸çš„å€‹æ•¸ï¼Œä¿‚æ•¸æ°¸é ä¸æœƒæ˜¯0ï¼Œä¸èƒ½ç”¨ä¾†åš feature reduction
- ç‚ºä»€éº¼åŠ å…¥ L1 / L2 å°±æœƒé€ æˆ Weight decay ?
    - L2 åœ¨é€²è¡Œåƒæ•¸æ›´æ–°çš„æ™‚å€™æœƒä¹˜ä¸Š(1âˆ’2Î·Î»)ï¼Œå…¶å€¼å°æ–¼1ï¼Œå¿…å®šæœƒä½¿åƒæ•¸éæ¸›ä¸¦å¾€0é è¿‘
- L1 + L2 çš„çµåˆ = Elastic Netï¼Œè§£æ±ºå…©è€…ç¼ºé»
![](https://i.imgur.com/YAfg2Pk.jpg)
![](https://i.imgur.com/BnVE25T.jpg)
![](https://i.imgur.com/Gg9SZ8m.jpg)

<center class="half">
    <img src="https://i.imgur.com/n3ADxGz.png" width="400">
    <img src="https://i.imgur.com/aAc4tGl.png" width="400">
</center>

#### Early Stopping
  - åˆ¤æ–·ä½•æ™‚éœ€è¦åœæ­¢è¨“ç·´ï¼Œåœ–ä¸­ã€€$d^*_{vc}$ ç‚ºæœ€ä½³åœæ­¢æ™‚é–“ 
    ![](https://i.imgur.com/K4Roa4W.png)
    
#### Dropout
  - åœ¨æ¯ä¸€å±¤éš¨æ©Ÿåœç”¨æŸéƒ¨åˆ†(p%)çš„ç¥ç¶“å…ƒ 
  - ç•¶é€™äº›åˆ©ç”¨ p% æ‰€è¨“ç·´å‡ºä¾†çš„æ¬Šé‡è¦æ‹¿åˆ° test data è©•ä¼°çš„æ™‚å€™ï¼Œå› ç‚º testing çš„æ™‚å€™ä¸ä½¿ç”¨ Dropoutï¼Œæ‰€ä»¥ testing æ™‚æ‰€æœ‰æ¬Šé‡è¦ä¹˜ä¸Š (1-p)%
![](https://i.imgur.com/YFeWGB7.png)


#### å…¶ä»– regularization æ–¹æ³•

- Stochastic Depth
- Label Smoothing
- Layer Scale

### Batch Normalization

[batch normalization in 3 level](https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338)
![](https://i.imgur.com/pppjMSE.png)
- Batch Normalization çš„ä½œæ³•æ˜¯å°æ¯ä¸€å€‹ mini-batch éƒ½é€²è¡Œæ­£è¦åŒ–åˆ°å¹³å‡å€¼ç‚º0ã€æ¨™æº–å·®ç‚º1çš„å¸¸æ…‹åˆ†ä½ˆ
  - **é©åˆä½¿ç”¨åœ¨ MLP æˆ–æ˜¯ CNN ä¹‹ä¸Šï¼Œä¸é©ç”¨åœ¨ RNN æˆ–æ˜¯ batch size å°çš„æƒ…æ³ä¸Š**ï¼Œå› ç‚ºå…¶ batch ä¸­çš„è³‡è¨Šä¸¦æ²’æœ‰è¾¦æ³•åæ˜ åœ¨å…¨å±€çš„çµ±è¨ˆåˆ†å¸ƒä¸Š
- é€™æ¨£å¯ä»¥æŠŠåˆ†æ•£çš„æ•¸æ“šçµ±ä¸€ï¼Œæœ‰åŠ©æ–¼æ¸›ç·©æ¢¯åº¦æ¶ˆå¤±ä»¥åŠè§£æ±º Internal Covariate Shift çš„å•é¡Œï¼ŒåŒæ™‚å¯ä»¥åŠ é€Ÿæ”¶æ–‚ï¼Œä¸¦ä¸”æœ‰æ­£å‰‡åŒ–çš„æ•ˆæœ (å¯ä»¥ä¸ä½¿ç”¨Dropout)
  - Î³ and Î² ç‚ºæ§åˆ¶ linear/affine transformation çš„å¯å­¸ç¿’åƒæ•¸ï¼Œåœ¨ [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d) ä¸­å¯é€é `affine` = true / false ä¾†é–‹é—œ
![](https://i.imgur.com/7eIU1Oc.jpg)

- ç¶“é Normalization ä¹‹å¾Œçš„æ•¸æ“šåœ¨é€šéæ¿€æ´»å‡½æ•¸å¾Œï¼Œå¯ä»¥å¾—åˆ°åˆ†ä½ˆè¼ƒç‚ºå¹³å‡çš„è¼¸å‡º
![](https://i.imgur.com/6zDexYJ.png)
![](https://i.imgur.com/NdzE2Rs.jpg)

å„ªé» : 
1. æ¸›ç·©æ¢¯åº¦æ¶ˆå¤±çš„å•é¡Œ
2. è§£æ±º Internal Covariate Shift çš„å•é¡Œ
3. åŠ é€Ÿæ¨¡å‹æ”¶æ–‚
4. å…·æœ‰æ­£å‰‡åŒ–æ•ˆæœ

ç¼ºé» :
ä¾†è‡ª [NFNets](https://arxiv.org/abs/2102.06171) çš„è«–é»
1. åœ¨è¨ˆç®—å¹³å‡æ•¸å’Œæ¨™æº–å·®æ™‚ï¼Œéœ€è¦å°‡å…¶å€¼ä¿å­˜åœ¨è¨˜æ†¶é«”ä¸­ï¼Œé™¤äº†æœƒå¢åŠ è¨˜æ†¶é«”ä½¿ç”¨é‡(è¨ˆç®—æˆæœ¬)ï¼Œä¹Ÿå¢åŠ äº†ç¶²è·¯è©•ä¼°æ¢¯åº¦çš„æ™‚é–“
2. é€ æˆæ¨¡å‹è¨“ç·´å’Œæ¨è«–æ™‚çš„å·®ç•°ï¼Œä»£è¡¨ BN å¼•å…¥äº†å¿…é ˆèª¿æ•´çš„éš±è—è¶…åƒæ•¸
3. ç ´å£äº† minibatch çš„è¨“ç·´è³‡æ–™é–“çš„ç¨ç«‹æ€§ï¼Œå› æ­¤åœ¨ minibatch ä¸­é¸æ“‡å“ªäº›æ¨£æœ¬è®Šå¾—å¾ˆé‡è¦

å› æ­¤é€ æˆ
1. é›£ä»¥é€²è¡Œåˆ†å¸ƒå¼è¨“ç·´ (å› ç‚º Train-test inconsistency å®¹æ˜“é€ æˆè³‡æ–™æ´©æ¼)
2. Batch size ä¸èƒ½å¤ªå°ï¼Œæœƒå¾ˆä¸ç©©å®šï¼Œé€šå¸¸BNåªèƒ½ç”¨åœ¨å¤§æ¨¡å‹ä¸Šï¼Œä¹Ÿå› æ­¤å°è‡´è¨±å¤šéœ€è¦å¤§é‡ gpu è¨˜æ†¶é«”çš„ tasks( Detection, segmentation, video) ç„¡æ³•ä½¿ç”¨ BN
   - æ­¤æ™‚å¯æ”¹ç”¨ [Group Normalization, 2018](https://arxiv.org/abs/1803.08494)

é™¤æ­¤ä¹‹å¤–ï¼Œåœ¨ [Rethinking â€œBatchâ€ in BatchNorm, 2021](https://arxiv.org/pdf/2105.07576.pdf) ä¸­ä¹Ÿæå‡ºæ›´å¤š BN çš„ç¼ºé»ï¼Œæ¯”å¦‚ batch çš„æ¦‚å¿µè¼ƒæ¨¡ç³Šå› æ­¤å¯¦ä½œæ–¹æ³•å¤šã€å¯¦ä½œä¸Šå®¹æ˜“æœ‰ bug ç­‰ç­‰

#### å¦‚ä½•æ”¹å–„ Batch Norm å•é¡Œ?

[Exploring Adaptive Gradient Clipping and NFNets](https://wandb.ai/ayush-thakur/nfnet/reports/Exploring-Adaptive-Gradient-Clipping-and-NFNets--Vmlldzo1MDc0NTQ)

- [NFNet, 2021](https://arxiv.org/abs/2102.06171) ä¸­æå‡ºäº†è‡ªé©æ‡‰æ¢¯åº¦ä¿®å‰ªï¼ˆAdaptive Gradient Clippingï¼ŒAGCï¼‰æ–¹æ³•ï¼Œæ˜¯åŸºæ–¼æ¢¯åº¦ç¯„æ•¸èˆ‡åƒæ•¸ç¯„æ•¸çš„å–®ä½æ¯”ä¾‹ä¾†è£åˆ‡æ¢¯åº¦
  - åˆ©ç”¨ history of gradient norms å»è¨­å®š clipping value
  - choose a percentile $p$ instead of a absolute value as clipping threshold
  - AGC å¯ä»¥è¨“ç·´æ›´å¤§ Batch size å’Œå¤§è¦æ¨¡æ•¸æ“šå¢å¼·çš„éæ­¸ä¸€åŒ–ç¶²çµ¡ï¼Œä½†è¨“ç·´ç©©å®šæ€§æœƒå° Î» ç‰¹åˆ¥æ•æ„Ÿ
![](https://i.imgur.com/GY10SpH.jpg)


### Layer Normalization, Instance Normalization, Group Normalization, 
[In-layer normalization techniques for training very deep neural networks](https://theaisummer.com/normalization/)
![](https://i.imgur.com/ZkM0SdM.png)
![](https://i.imgur.com/dERQg9m.jpg)


#### Layer normalization
- é¦–æ¬¡æ–¼ [2016å¹´](https://arxiv.org/abs/1607.06450) è¢«æå‡º
  - ä¸€é–‹å§‹æ˜¯ç”¨ä¾†è™•ç† vector (å¤§å¤šæ˜¯ RNN çš„è¼¸å‡º)ï¼Œä½†ä¸€ç›´é»˜é»˜ç„¡åï¼Œç›´åˆ° transformers å‡ºç¾ä¹‹å¾Œæ‰åˆè¢«ç©æ¥µè¨è«–
  - **é™¤äº† RNN èˆ‡ Attention ä»¥å¤–ä¹Ÿé©åˆä½¿ç”¨åœ¨ batch size è¼ƒå°çš„ä»»å‹™ä¸Š**
- è·¨è¶Šæ‰€æœ‰ channels ä»¥åŠ spatial dimension å°‡æ¯ä¸€å€‹ feature çš„ activations æ­£è¦åŒ–åˆ° zero mean and unit variance
  - **ç¨ç«‹æ–¼ batch ä¹‹å¤– (Batch independent)** ç‚ºå…¶æœ€é‡è¦çš„ç‰¹æ€§
![](https://i.imgur.com/m6iSv4Q.png)

ç®—æ³•:
- Î³ and Î² ç‚ºæ§åˆ¶ linear/affine transformation çš„å¯å­¸ç¿’åƒæ•¸ï¼Œåœ¨ [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm) ä¸­å¯é€é `elementwise_affine` = true / false ä¾†é–‹é—œ
![](https://i.imgur.com/6x1Rj09.jpg)

#### Instance normalization
- èˆ‡ LN ä¸€æ¨£åœ¨ [2016](https://arxiv.org/abs/1607.08022) è¢«æå‡ºï¼ŒIN ä¸»è¦æ‡‰ç”¨åœ¨ç‰¹å¾µè¼ƒå¯†é›†çš„é›»è…¦è¦–è¦ºé ˜åŸŸä»¥åŠæ¯ä¸€å€‹ pixels éƒ½æœ‰ç”¨è™•çš„æ¼”ç®—æ³•ä¸Š (ä¾‹å¦‚ GAN)
  - **ä¸å»ºè­°è¢«ç”¨åœ¨ (1) MLP æˆ– RNNï¼Œå› ç‚ºå…¶ä¸€å€‹ channel ä¸Šåªæœ‰ä¸€å€‹è³‡æ–™ (2) feature map è¼ƒå°æ™‚**
- åªåœ¨å–®ä¸€å€‹ channelã€å–®ä¸€å€‹æ¨£æœ¬ä¸­æ¯å€‹ feature çš„ç©ºé–“ç¶­åº¦ä¹‹ä¸­è¨ˆç®—
  - **ç¨ç«‹æ–¼ channel å’Œä¸åŒæ¨£æœ¬ä¹‹å¤– (independent for each channel and sample)**
![](https://i.imgur.com/Oezntqt.png)
- æœ¬è³ªä¸Šå°±æ˜¯åœ¨ normalize featuresï¼Œå› æ­¤å¯ç”¨ä¾†æ”¹è®Šä¸€å¼µåœ–ç‰‡çš„é¢¨æ ¼ (é€éå¯å­¸ç¿’åƒæ•¸ $\gamma$ å’Œ $\beta$)

ç®—æ³•:
  - å…¶å¯¦å°±åªæ˜¯æŠŠ BN ä¸­çš„ $\mu$ å’Œ $\sigma$ å»æ‰ $N$
  - Î³ and Î² ç‚ºæ§åˆ¶ linear/affine transformation çš„å¯å­¸ç¿’åƒæ•¸ï¼Œåœ¨ [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html#torch.nn.InstanceNorm1d) ä¸­å¯é€é `affine` = true / false ä¾†é–‹é—œ
![](https://i.imgur.com/2HlNxGT.jpg)

#### Group normalization
- æ–¼ [2018](https://arxiv.org/abs/1803.08494) è¢«æå‡ºï¼Œæ‡‰ç”¨åœ¨åœ–ç‰‡åˆ†é¡ã€ç‰©ä»¶åµæ¸¬ã€ç‰©ä»¶åˆ†å‰²ç­‰è¦–è¦ºä»»å‹™ä¸Š
- GN æŠŠchannel åˆ†æˆ num_group å€‹ group ä¸¦åˆ†é–‹å€‹åˆ¥å°å…¶åš normalization (ç®—å‡ºå€‹åˆ¥çš„ mean å’Œ var)
  - ä¸€æ¨£**ç¨ç«‹æ–¼ batch ä¹‹å¤–**
  - num_group ç‚ºä¸€å€‹å¯èª¿çš„è¶…åƒæ•¸ï¼Œnum_group=1 å°±æ˜¯ LN
![](https://i.imgur.com/KwOfHr7.png)
- åœ¨å„ç¨®ä¸åŒçš„ batch size ä¸‹ï¼ŒGN æ“æœ‰æ¯” BN æ›´ç©©å®šçš„æº–ç¢ºç‡
![](https://i.imgur.com/HnLUCEv.jpg)

ç®—æ³•:
  - $G$ = num_groupï¼Œè¶…åƒæ•¸ï¼Œè«–æ–‡ä¸­ä»¥ G=32 åšç‚ºé è¨­åƒæ•¸
  - $C/G$ =  number of channels per group
  - $S_i$ = ç¬¬ $i$ å€‹ Set
  - GN computes Âµ and Ïƒ along the (H, W) axes and along a group of C/G channels
  - Î³ and Î² ç‚ºæ§åˆ¶ linear/affine transformation çš„å¯å­¸ç¿’åƒæ•¸ï¼Œåœ¨ [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html#torch.nn.GroupNorm) ä¸­å¯é€é `affine` = true / false ä¾†é–‹é—œ
![](https://i.imgur.com/KrqMLGd.jpg)
![](https://i.imgur.com/QXQIpCp.jpg)


### Learning Rate Schedules

[Pytorch optIM](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)

[Guide to Pytorch learning rate scheduling](https://www.kaggle.com/isbhargav/guide-to-pytorch-learning-rate-scheduling)

#### LinearLR: ç”¨å…©å€‹ multiplicative factorï¼Œä¸€å€‹ä½œç‚ºèµ·å§‹ï¼Œå¦ä¸€å€‹ä½œç‚ºçµ‚é»ä¾†ç·šæ€§è¡°æ¸›å­¸ç¿’ç‡ç›´åˆ° total_inters
- `scheduler = LinearLR(self.opt, start_factor=0.5, total_iters=4)`

#### ExponentialLR: ç”¨ä¸€å€‹å›ºå®šçš„ Multiplicative factor `gamma` ä¾†è¡°æ¸›æ¯å€‹ epoch çš„å­¸ç¿’ç‡
- `torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)`
![](https://i.imgur.com/0wYjKqa.png)

#### CosineAnnealing: é€éé¤˜å¼¦é€€ç«å¯ä»¥è®“å­¸ç¿’ç‡å…ˆç·©é™->å†é©Ÿé™->æœ€å¾Œç·©é™
- `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0)`
    - T_max: æœ€å¤šè¿­ä»£æ¬¡æ•¸
    - eta_min: æœ€å°å­¸ç¿’ç‡
![](https://i.imgur.com/Ay8msGd.png)
- å­¸ç¿’ç‡ä¸€å€‹é€±æœŸ(å®Œæˆä¸€æ¬¡ epochs=50 çš„è¨“ç·´)çš„è®ŠåŒ–
![](https://i.imgur.com/7GP73Bf.png)
- Implementation
```python=
global_step = min(global_step, decay_steps)
cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
decayed = (1 - alpha) * cosine_decay + alpha
decayed_learning_rate = learning_rate * decayed
```

#### CosineAnnealingWarmRestarts: é€±æœŸæ€§çš„ä½¿ç”¨ cosine annealing
- `torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0)`
   - T_0: First restart å¾Œçš„è¿­ä»£æ¬¡æ•¸
   - T_mult: åœ¨ restart å¾Œå¢åŠ è¿­ä»£æ¬¡æ•¸çš„ multiplicative factorï¼Œé è¨­ç‚º1
   - eta_min: æœ€å°å­¸ç¿’ç‡
![](https://i.imgur.com/iV5NzvD.png)

- é€±æœŸæ€§çš„ä½¿ç”¨ cosine annealing æ‰€å½¢æˆçš„å­¸ç¿’ç‡è®ŠåŒ–

![](https://i.imgur.com/t21inCG.png)
#### CyclicLR: æ ¹æ“š cyclical learning rate policy (CLR) åœ¨ä¸€å€‹è¨­å®šå¥½çš„å€é–“å…§ä»¥ constant frequency ä¾†å¾ªç’°å­¸ç¿’ç‡
- `torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr)`
- base_lr å’Œ max_lr æœƒæ±ºå®šå­¸ç¿’ç‡å¾ªç’°çš„å€é–“
- åˆ†ç‚ºä¸‰ç¨®å¾ªç’°æ–¹å¼ï¼štriangular, triangular2, exp_range, é è¨­åƒæ•¸ç‚ºmode='triangular'
1. triangular
![](https://i.imgur.com/lpGnSXV.png)
2. triangular2
![](https://i.imgur.com/7LQzvMy.png)
3. exp_range
![](https://i.imgur.com/duOZRRd.png)

#### OneCycleLR: æ ¹æ“š 1cycle learning rate policy ä¾†å¾åˆå§‹å­¸ç¿’ç‡é€€ç«åˆ°ä¸€å€‹æœ€å¤§å­¸ç¿’ç‡ï¼Œå†å¾é€™å€‹æœ€å¤§å­¸ç¿’ç‡é€€ç«åˆ°ä¸€å€‹æ¯”åˆå§‹å­¸ç¿’ç‡é‚„å°çš„æœ€å°å­¸ç¿’ç‡
- `torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)`
- total_steps = epochs * steps_per_epochï¼Œè¦å˜›å°±æŒ‡å®š step_per_epoch è¦å˜›å°±æŒ‡å®š total_steps
- é€€ç«ç­–ç•¥æœ‰å…©ç¨®å¯ä»¥é¸: linear å’Œ consineï¼Œé è¨­åƒæ•¸ç‚º anneal_strategy='cos'
1. consine
![](https://i.imgur.com/rgV2mHl.png)
2. linear
![](https://i.imgur.com/dkhKYfj.png)

### Loss
#### Regression Loss
- $MSE, L2 loss = {\displaystyle\sum_{i=1}^{D}(x_i-y_i)^2}$ 
   - MSE è¨ˆç®—æ–¹ä¾¿ä½†é‡åˆ° outlier æœƒè¼ƒä¸ç©©å®š 
- $RMSE = {\displaystyle\sqrt(MSE) = \sqrt(\sum_{i=1}^{D}(x_i-y_i)^2)}$
- ${\displaystyle {\mbox{MAE}} = \sum_{i=1}^{D}|x_i-y_i|}$
  - MAE è¼ƒèƒ½è™•ç† outlier ä½†æ”¶æ–‚é€Ÿåº¦è¼ƒæ…¢ 
- $Huber loss = L_{\delta}=
    \left\{\begin{matrix}
        \frac{1}{2}(y - \hat{y})^{2} & if \left | (y - \hat{y})  \right | < \delta\\
        \delta ((y - \hat{y}) - \frac1 2 \delta) & otherwise
    \end{matrix}\right.$
  - Itâ€™s less sensitive to outliers than the MSE as it treats error as square only inside an interval.
  - HuberLoss çš„å­˜åœ¨å°±æ˜¯å¸Œæœ›èƒ½é™ä½MSEå°Outlier çš„ä¸ç©©å®šæ€§ï¼Œä¸¦æå‡ MAE çš„æ”¶æ–‚é€Ÿåº¦
 - ${\displaystyle {\mbox{MAPE}}={\frac {100}{n}}\sum _{t=1}^{n}\left|{\frac {A_{t}-F_{t}}{A_{t}}}\right|}$
     - $A_t$æ˜¯å¯¦éš›å€¼
     - $F_t$æ˜¯é æ¸¬å€¼
     - è‹¥æœ‰è³‡æ–™æœƒç­‰æ–¼0ä¸å¯ç”¨
     - ä¸»è¦æ˜¯ç”¨ä¾†æ¯”è¼ƒæŸå…©è€…ï¼Œè€ƒæ…®çš„æ˜¯ç›¸å°èª¤å·®
     - æ¯”å¦‚å…©é–“é£²æ–™åº—è³£çš„é£²æ–™æ•¸ï¼Œé€™æ¨£æ‰å¯ä»¥æŠŠçœŸæ­£çš„å¯¦éš›å€¼ä¹Ÿè€ƒæ…®é€²å»ï¼Œæ‰ä¸æœƒæŠŠ
å…¶ä»–é‚„æœ‰ MSPE, MSLE...ç­‰ç­‰

##### Summary

MSE & MAE: è€ƒæ…®çš„æ˜¯çµ•å°èª¤å·®ï¼ŒMSPE & MAPE: è€ƒæ…®çš„æ˜¯ç›¸å°èª¤å·®
- MAE: æœ‰ç•°å¸¸å€¼çš„æƒ…æ³ï¼Œå¦‚æœä¸æƒ³è¦é€™äº›ç•°å¸¸å€¼å½±éŸ¿æ¨¡å‹å¯ä»¥ç”¨
- MSE: æœ‰ä¸€é»ç•°å¸¸å€¼çš„æƒ…æ³ï¼Œå¦‚æœæƒ³è¦åŒ…å«é€™äº›ç•°å¸¸å€¼å¯ä»¥ç”¨
- MSPE: å…·æœ‰æ¬Šé‡æ¦‚å¿µçš„MSE
- MAPE: å…·æœ‰æ¬Šé‡æ¦‚å¿µçš„MAE
- MSLE: å–å°æ•¸(log)çš„MSE

#### Classification Loss

- $Cross Entropy = -{(y\log(p) + (1 - y)\log(1 - p))}-\sum_{c=1}^My_{o,c}\log(p_{o,c})$
    - M - number of classes
    - log - the natural log
    - y - binary indicator (0 or 1) if class label c is the correct classification for observation o
    - p - predicted probability observation o is of class c
- $Negative Loglikelihood = NLL(y) = -{\log(p(y))}\min_{\theta} \sum_y {-\log(p(y;\theta))}\max_{\theta} \prod_y p(y;\theta)$
- $Hinge loss = max(0, 1 - y \cdot \hat{y})$
  - Used in SVM
- KL/JS divergence
![](https://i.imgur.com/ZwmeYO9.jpg)


### Data Splitting
[Data Splitting for Model Evaluation](https://towardsdatascience.com/data-splitting-for-model-evaluation-d9545cd04a99)

#### Imbalanced Dataset
*Question: What problem may arise if we randomly split a dataset that has 99% negative class and 1% positive class?*
- æœ€å¥½ä½¿ç”¨ stratify çš„æ–¹å¼ä¾†å°‡è³‡æ–™åˆ‡å‰²ç‚º class æ•¸é‡ä¸€æ¨£çš„ train/test split
- å³ä½¿ä¸æ˜¯ä¸å¹³è¡¡è³‡æ–™é›†ä¹Ÿå¯ä»¥åšï¼Œé€™æ¨£å¯ä»¥ç¢ºä¿ training å’Œ testing dataset æœ‰é¡ä¼¼çš„è³‡æ–™åˆ†å¸ƒ
```python=
from sklearn.model_selection import train_test_split
# æ ¹æ“š y çš„é¡åˆ¥æ•¸ä¾†åˆ‡å‰²å‡ºæ¯å€‹é¡åˆ¥éƒ½ä¸€æ¨£çš„ train å’Œ test
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.33, random_state=42, stratify=y)
```

#### Limited Data
*Question: In experiments with limited data, what could be a possible model evaluation issue if we perform a simple train-test split?*
- å¯ä»¥åˆ©ç”¨ K-Fold CV ä¾†è§£æ±ºè³‡æ–™è¼ƒç‚ºä¸è¶³çš„æƒ…æ³
![](https://i.imgur.com/s2Vozh8.png)

#### Feature Engineering Leakage
*Question: Whatâ€™s wrong with standardizing on the whole data before doing a train-test split?*
- å‡å¦‚æˆ‘å€‘åœ¨åšè³‡æ–™åˆ†å‰²ä¹‹å‰å°±å°æ•´å€‹è³‡æ–™åšæ¨™æº–åŒ–ï¼Œå°±æœƒé€ æˆ data leakageï¼Œå› ç‚ºæˆ‘å€‘æœƒé€é mean ä»¥åŠ std å°‡ testing data çš„ç‰¹æ€§æ´©æ¼çµ¦ training data
- é™¤æ­¤ä¹‹å¤–å¦ä¸€å€‹å¸¸è¦‹çš„å‰‡æ˜¯å°æ•´å€‹è³‡æ–™åš Hot deck imputation

#### Group Leakage
*Question: If there are patient overlaps between the train, validation, and test sets, why might the model performance be greatly overestimated?*

- è¦æ”¹ç”¨ object identifier ä¾†åˆ‡å‰²è³‡æ–™ï¼Œè€Œä¸æ˜¯ data pointsï¼Œé€™æ¨£åŒä¸€å€‹ç‰©ä»¶æ‰€ç”¢ç”Ÿçš„è³‡æ–™åªæœƒå­˜åœ¨æ–¼ trainingã€€data æˆ–æ˜¯ testing data

#### Time Leakage
*Question: Any issue with randomly splitting a time series data?*

- æŒ‰ç…§æ™‚é–“é †åºé€²è¡Œ train/test åˆ‡å‰² (ä»¥è¼ƒèˆŠè³‡æ–™ä½œç‚º train, è¼ƒæ–°è³‡æ–™ä½œç‚º test)ï¼Œæˆ–æ˜¯ä½¿ç”¨ walk-forward validation

## Tensorflow / Pytorch

[æµ…è°ˆ PyTorch ä¸­çš„ tensor åŠä½¿ç”¨](https://zhuanlan.zhihu.com/p/67184419)
[PyTorch æŒ‡å—ï¼š17å€‹æŠ€å·§è®“ä½ çš„æ·±åº¦å­¸ç¿’æ¨¡å‹è¨“ç·´è®Šå¾—é£›å¿«ï¼
](https://bangqu.com/Ya9W74.html)


- torch.no_grad()
  - åœ¨åš inference çš„æ™‚å€™å¯ä»¥ä¸è¿½è¹¤æ¨¡å‹çš„åƒæ•¸ï¼Œå› æ­¤å¯ä»¥å°‡å…¶åŒ…åœ¨no_grad()åº•ä¸‹æ¸›å°‘æµªè²»çš„è¨ˆç®—é‡
```python=
# requires_grad æ˜¯ç”¨ä¾†ç¢ºèªä¸€å€‹tensoræ˜¯å¦éœ€è¦æ±‚å°
x = torch.randn(3, requires_grad = True)
print(x.requires_grad)
# True
print((x ** 2).requires_grad)
# True

with torch.no_grad():
    print((x ** 2).requires_grad)
    # False

print((x ** 2).requires_grad)
# True

```
- torch.ones
```python=
# æ¨¡æ“¬8å¼µåœ–ç‰‡ï¼Œæ¯å¼µåœ–ç‰‡çš„å¤§å°æ˜¯10*10
# æ¯å€‹ç›®æ¨™éƒ½åˆå§‹åŒ–ç‚º1
images = torch.ones(8, 3, 10, 10)
targets = torch.ones(8, dtype=torch.long)
```

## Numpy / Pandas

[Python Numpy and Matrices Questions for Data Scientists](https://towardsdatascience.com/python-numpy-and-matrices-questions-for-data-scientists-167af1c9d3a4)

- pd.melt
  - æŠŠè¤‡é›œçš„è³‡æ–™æ‹†è§£æ‰è®Šæˆç°¡å–®çš„è¡¨æ ¼å¼è³‡æ–™ 
  - `pandas.melt(frame, id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None)`
	  - id_vars: å¯ä½¿ç”¨ tupleã€listã€æˆ– ndarrayï¼Œç”¨ä»¥è¨­å®šä¸æƒ³è¦è¢«è½‰æ›çš„æ¬„ä½
	  - value_vars: å¯ä½¿ç”¨ tupleã€listã€æˆ– ndarrayï¼Œç”¨ä»¥è¨­å®šæƒ³è¦è¢«æ‹†è§£çš„æ¬„ä½ã€‚ å¦‚æœçœç•¥å‰‡æ‹†è§£å…¨éƒ¨æ¬„ä½
	  - var_name : è½‰æ›å¾Œ id çš„åç¨±ã€‚å¦‚æœçœç•¥å‰‡è¨­å®šç‚ºåŸæœ¬ DataFrame çš„æ¬„ä½åç¨±æˆ–æ˜¯ variableã€‚
	  - value_name : è½‰æ›å¾Œ value æ¬„ä½çš„åç¨±ã€‚å¦‚æœçœç•¥å‰‡é¡¯ç¤ºåŸæœ¬ DataFrame çš„æ¬„ä½åç¨±æˆ– valueã€‚
	  - col_level : å¯ä½¿ç”¨ intã€stringã€‚å¦‚æœ columns æ˜¯ MultiIndexï¼Œå‰‡ä½¿ç”¨è©²åƒæ•¸ä¾†é€²è¡Œé¸æ“‡ã€‚

## Sklearn

- [News in Scikit-learn 1.0](https://hackmd.io/7RlOjqQfQp2HlNpPc-i6ZQ)

- [Balanced_accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html)
	- è™•ç†åˆ†é¡å›æ­¸ä»»å‹™çš„ä¸å¹³è¡¡è³‡æ–™æ™‚éœ€æ”¹æˆbalanced_accuracy
    - balanced accuracy = (sensitivity + specificity) / 2
    	- sensitivity (Recall) = TP / (TP+FN) å¯¦éš›ç‚ºé™½æ€§çš„æ¨£æœ¬ä¸­ï¼Œåˆ¤æ–·ç‚ºé™½æ€§çš„æ¯”ä¾‹zz
    	- specificity = TN / (FP+TN) å¯¦éš›ç‚ºé™°æ€§çš„æ¨£æœ¬ä¸­ï¼Œåˆ¤æ–·ç‚ºé™°æ€§çš„æ¯”ä¾‹
	- accuracy = (TP+TN) / (TP+TN+FP+FN)

èˆ‰ä¾‹ä¾†èªª

|          | Predicted Positive | Predicted Negative |
| -------- | -------- | -------- |
| Actual Positive     | 1        | 8                 |
| Actual Negative     | 2        | 989               |

æ­¤æ™‚ accuracy = 990 / 1000 = 99%
è€Œ balanced accuracy = (((1/(1 + 8)) + ( 989/(2 + 989))) / 2 = 55.5%

## SQL

### CheatSheet

![](https://i.imgur.com/atEPqS0.jpg)

### JOIN

- INNER JOIN (äº¤é›†): åƒ…é¡¯ç¤ºå…©è³‡æ–™è¡¨å°æ‡‰æ¬„ä½ä¸­å€¼ç›¸åŒçš„æ¬„ä½
- LEFT JOIN : ä¸²è¯å…©å€‹è³‡æ–™è¡¨ä¸­å°æ‡‰æ¬„è³‡æ–™æ™‚ï¼Œä»¥è³‡æ–™è¡¨1çš„è³‡æ–™ç‚ºä¸»ï¼Œè‹¥è³‡æ–™å­˜åœ¨æ–¼è³‡æ–™è¡¨1ï¼Œä½†è³‡æ–™è¡¨2æ²’æœ‰å°æ‡‰å€¼æ™‚ï¼Œä»é¡¯ç¤ºè³‡æ–™è¡¨1ä¸­çš„è³‡æ–™ã€‚
- RIGHT JOIN ä¸²è¯å…©å€‹è³‡æ–™è¡¨ä¸­å°æ‡‰æ¬„è³‡æ–™æ™‚ï¼Œä»¥è³‡æ–™è¡¨2çš„è³‡æ–™ç‚ºä¸»ï¼Œè‹¥è³‡æ–™å­˜åœ¨æ–¼è³‡æ–™è¡¨2ï¼Œä½†è³‡æ–™è¡¨1æ²’æœ‰å°æ‡‰å€¼æ™‚ï¼Œä»é¡¯ç¤ºè³‡æ–™è¡¨2ä¸­çš„è³‡æ–™ã€‚

![](https://i.imgur.com/a6YSlmW.jpg)


### å¸¸è¦‹é¢è©¦é¡Œ
[5 Common Problems](https://towardsdatascience.com/5-common-sql-interview-problems-for-data-scientists-1bfa02d8bae6)
#### Second Highest Salary
Question : æ‰¾å‡ºSalaryç¬¬äºŒé«˜çš„æ˜¯å¤šå°‘
```
+----+--------+
| Id | Salary |
+----+--------+
| 1  | 100    |
| 2  | 200    |
| 3  | 300    |
+----+--------+
```
Ans :
- è§£ä¸€ : åˆ©ç”¨ MAX() æ‰¾å‡ºä¸ç­‰æ–¼ MAX çš„MAXå€¼
```sql=
SELECT MAX(salary) AS SecondHighestSalary
FROM Employee
WHERE salary != (SELECT MAX(salary) FROM Employee)
```
- è§£äºŒ : åˆ©ç”¨  IFNULL æ‰¾å‡ºéNULLå€¼ä»¥åŠç”¨ OFFSET æ‰¾å‡ºç¬¬äºŒé«˜çš„å€¼
```sql=
SELECT
    IFNULL(
        (SELECT DISTINCT Salary
        FROM Employee
        ORDER BY Salary DESC
        LIMIT 1 OFFSET 1 --é™åˆ¶åªå–ä¸€ç­†ä¸”ç•¥éç¬¬ä¸€ç­†
        ), null) as SecondHighestSalary
FROM Employee
LIMIT 1
```

#### Duplicate Emails
Question : æ‰¾å‡ºæ‰€æœ‰åœ¨ table `person` ä¸­æœ‰é‡è¤‡çš„ eamils
```
+----+---------+
| Id | Email   |
+----+---------+
| 1  | a@b.com |
| 2  | c@d.com |
| 3  | a@b.com |
+----+---------+
```
Ans :
- è§£ä¸€ : ç”¨ COUNT 
```sql=
SELECT Email
FROM (
  SELECT Email, count(Email) AS count
  FROM Person
  GROUP BY Email
) as email_count
WHERE count > 1
```
- è§£äºŒ : ç”¨ HAVING
```sql=
SELECT Email
FROM Person
GROUP BY Email
HAVING count(Email) > 1
```
#### Rising Temperature
Question: çµ¦å®šä¸€å€‹`Wether`Table, æ‰¾å‡ºæ‰€æœ‰å‰ä¸€å¤©çš„æº«åº¦æ¯”ä»Šå¤©é«˜çš„ DATE_Id

```
+---------+------------------+------------------+
| Id(INT) | RecordDate(DATE) | Temperature(INT) |
+---------+------------------+------------------+
|       1 |       2015-01-01 |               10 |
|       2 |       2015-01-02 |               25 |
|       3 |       2015-01-03 |               20 |
|       4 |       2015-01-04 |               30 |
+---------+------------------+------------------+
```
Ans : 
- åˆ©ç”¨ DATEDIFF(startdate, enddate) ä¾†ç¢ºå®šä»Šå¤©å’Œæ˜¨å¤©æ˜¯å¦å·®ä¸€å¤©
```sql=
SELECT DISTINCT a.Id
FROM Weather a, Weather b
WHERE a.Temperature > b.Temperature
AND DATEDIFF(a.Recorddate, b.Recorddate) = 1
```

#### Department Highest Salary
Question : çµ¦å®š`Employee`Table å’Œ `Department`Table, æ‰¾å‡ºæ¯ä¸€å€‹ Department ä¸­è–ªæ°´æœ€é«˜çš„äººæ˜¯èª°ã€ä½å“ªå’Œå…¶è–ªæ°´

```
Employee
| Id | Name  | Salary | DepartmentId |
+----+-------+--------+--------------+
| 1  | Joe   | 70000  | 1            |
| 2  | Jim   | 90000  | 1            |
| 3  | Henry | 80000  | 2            |
| 4  | Sam   | 60000  | 2            |
| 5  | Max   | 90000  | 1            |
+----+-------+--------+--------------+
```
```
Department
+----+----------+
| Id | Name     |
+----+----------+
| 1  | IT       |
| 2  | Sales    |
+----+----------+
```
```
Expect Output
+------------+----------+--------+
| Department | Employee | Salary |
+------------+----------+--------+
| IT         | Max      | 90000  |
| IT         | Jim      | 90000  |
| Sales      | Henry    | 80000  |
+------------+----------+--------+
```

Ans : 

```sql=
SELECT Employee.Name AS 'Employee',
       Department.Name AS 'Department',
       Salary
FROM Employee
INNER JOIN Department ON Employee.DepartmentId = Department.Id
WHERE (DepartmentId, Salary)
IN ( SELECT-- SELECT Highest Salary and Id in Department
         DepartmentId, MAX(Salary)
     FROM
         Employee
     GROUP BY DepartmentId
)
```
#### Exchange Seat
Question : çµ¦å®šä¸€å€‹ `seat` Table, å…¶ä¸­çš„ id æ˜¯ä¸€å€‹é€£çºŒå¢åŠ çš„å€¼, å¸Œæœ›æŠŠåº§ä½æ”¹æˆåŸæœ¬ç›¸é„°çš„äººä¸å†ç›¸é„°
- æç¤º: è‹¥åº§ä½æ•¸æ˜¯å¥‡æ•¸, å‰‡ä¸å¿…æ”¹è®Šæœ€å¾Œä¸€å€‹äººçš„åº§ä½
```
+---------+---------+
|    id   | student |
+---------+---------+
|    1    | Abbot   |
|    2    | Doris   |
|    3    | Emerson |
|    4    | Green   |
|    5    | Jeames  |
+---------+---------+
```
Ans : 
```sql=
SELECT 
    CASE 
       -- åˆ¤æ–·æ˜¯å¦ç‚ºå¥‡æ•¸å€‹ row, è‹¥æ˜¯, å‰‡æœ€å¾Œä¸€å€‹åº§ä½ä¸å‹•
       WHEN((SELECT MAX(id) FROM seat)%2 = 1) 
       AND id = (SELECT MAX(id) FROM seat) THEN id
       -- å°æ¯ä¸€å€‹å¥‡æ•¸åº§ä½éƒ½+1 (1,3,5) -> (2,4,6)
       WHEN id%2 = 1 THEN id + 1
       -- å¶æ•¸åº§ä½-1 (2,4) -> (1,3)
       ELSE id - 1
    END AS id, student
FROM seat
ORDER BY id
```

## Basic ideas ( Statistic related )

### å¸¸æ…‹æ€§æª¢æ¸¬ ( Normality Testing )
- ç”¨ä¾†æª¢æ¸¬è³‡æ–™æ˜¯å¦ç‚ºå¸¸æ…‹åˆ†å¸ƒçš„å„ç¨®æ–¹æ³•

#### å¸¸æ…‹åˆ†å¸ƒçš„ç‰¹æ€§

- å¹³å‡å€¼ã€ä¸­ä½æ•¸å’Œçœ¾æ•¸ï¼Œä¸‰è€…æ˜¯åŒä¸€å€‹å€¼
  - è‹¥ç‚ºæ¨™æº–å¸¸æ…‹åˆ†å¸ƒï¼Œå…¶ $\mu =0$ï¼Œ$\sigma ^{2} = 1$
- å¤§ç´„æœ‰ 68% çš„è§€æ¸¬å€¼æœƒè½åœ¨ä¸­å¤®å·¦å³äºŒå´çš„ä¸€å€‹æ¨™æº–å·® Ïƒ ä¹‹å…§ï¼Œ95% çš„è§€æ¸¬å€¼æœƒè½åœ¨äºŒå€‹æ¨™æº–å·®ä¹‹å…§
- å¸¸æ…‹æ›²ç·šä»¥å¹³å‡å€¼ Î¼ ç‚ºä¸­å¿ƒï¼Œå·¦å³å…©å´æ­£è² ä¸€å€‹æ¨™æº–å·® Ïƒ çš„åœ°æ–¹ï¼Œå³æ›²ç·šä¸Šæ‰€è¬‚çš„åæ›²é» (infection point)
- ç†è«–ä¸Šé€™å€‹æ›²ç·šæœƒå‘äºŒå€‹å°¾ç«¯ç„¡é™å»¶ä¼¸
- å¸¸æ…‹åˆ†å¸ƒçš„è³‡è¨Šç†µåœ¨æ‰€æœ‰çš„å·²çŸ¥å‡å€¼åŠè®Šç•°æ•¸çš„åˆ†å¸ƒä¸­æœ€å¤§

#### æœ‰ä»€éº¼æ˜¯å»ºç«‹åœ¨å¸¸æ…‹åˆ†å¸ƒçš„å‡è¨­ä¹‹ä¸Š? 

åªè¦æ•´é«”è³‡æ–™å¤ å¤šï¼Œé©ç”¨æ–¼ä¸­å¤®æ¥µé™å®šç†çš„è©±ï¼Œæ•´é«”è³‡æ–™åˆ†å¸ƒå¯ä»¥ä¸ç¬¦åˆï¼Œåªè¦sample meanç¬¦åˆå¸¸æ…‹åˆ†å¸ƒå°±å¥½
- æ‰€æœ‰çš„æœ‰æ¯æ•¸æ–¹æ³• (Parametric Statistical Methods)éƒ½å»ºç«‹åœ¨å¸¸æ…‹åˆ†å¸ƒçš„å‡è¨­ä¹‹ä¸Š
  - åŒ…å« t-testã€ANOVAã€Pearson coefficient of correlation
  [Parametric and Nonparametric: Demystifying the Terms 
](https://www.mayo.edu/research/documents/parametric-and-nonparametric-demystifying-the-terms/doc-20408960)
![](https://i.imgur.com/32SHeob.jpg)
- åœ¨ç”¨ä»»ä½• parametric statistical tests ä¹‹å‰éƒ½æ‡‰è©²å…ˆæ¸¬è©¦ normalityï¼Œå¦å‰‡å°±åªèƒ½ä½¿ç”¨  non-parametric  statistical tests
#### å¸¸æ…‹æ€§æª¢æ¸¬å¯åˆ†ç‚ºåœ–å½¢æª¢é©—å’Œçµ±è¨ˆæª¢é©—å…©ç¨®

1. åœ–å½¢æª¢é©— : ç”¨ Histogramã€Boxplotã€Q-Q plot çœ‹å‡ºè³‡æ–™çš„åˆ†å¸ƒæƒ…å½¢ï¼Œä¹Ÿå¯ç”¨ä¾†æ¯”å°è·Ÿæ¨™æº–æƒ…æ³çš„å·®åˆ¥
   - Hisogram : bell-shaped
   ![](https://i.imgur.com/oy6Q3M1.jpg)
   - [Boxplot](https://www.simplypsychology.org/boxplots.html) : ä¸­ä½æ•¸åœ¨æ­£ä¸­é–“
![](https://i.imgur.com/yIWYW48.jpg)
   - Q-Q Plot : è³‡æ–™å‘ˆç¾æœƒè¿‘ä¼¼ y=x
![](https://i.imgur.com/iB7Tm6Q.jpg)


2. çµ±è¨ˆæª¢é©— : ç”¨ Shapiro-Wilk testã€Kolmogorov-Smirnov testã€Anderson-Darling testã€D'Agostino's K-squared testç­‰æ¸¬é©—å»æª¢æ¸¬
[machinelearning mastery å¯¦ä½œ](https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/)
   - **Shaporio-Wilk test** : è³‡æ–™å°(n<50)çš„æ™‚å€™è¡¨ç¾æœ€å¥½ï¼Œä¸éè³‡æ–™æ›´å¤šä¹Ÿå¯ä»¥ä½¿ç”¨ï¼Œè³‡æ–™éå¸¸æ…‹çš„æƒ…æ³ä¹Ÿå¾ˆé©åˆ   
   - **Kolmogorov-Smirnov test (K-S test)** : é©ç”¨æ–¼è³‡æ–™å¤§(n>50)ã€åŸºæ–¼ cdf å› æ­¤èƒ½å°ä¸­å¤®çš„è³‡æ–™å¯†åº¦æ›´æ•æ„Ÿ
   - **Anderson-Darling test (A-D test)**: K-S test çš„ä¿®æ­£ç‰ˆæœ¬ï¼Œèƒ½å°å°¾ç«¯çš„è³‡æ–™å¯†åº¦æ›´æ•æ„Ÿï¼Œé€šå¸¸æ¯”K-S testæ›´æ¨è–¦ä½¿ç”¨
   - **D'Agostino's K-squared test** : åˆ©ç”¨è³‡æ–™çš„ Skewness å’Œ Kurtosis ä¾†åˆ¤æ–·æ˜¯å¦ç‚ºå¸¸æ…‹åˆ†å¸ƒ
      - [Skewness (ååº¦)](https://zh.wikipedia.org/zh-tw/%E5%81%8F%E5%BA%A6) æ˜¯åœ¨è¡¡é‡è³‡æ–™åˆ†å¸ƒçš„ä¸å°ç¨±æ€§ 
        - æ­£åæ…‹(å³åæ…‹)è³‡æ–™é›†ä¸­åœ¨å·¦é‚Šï¼Œè² åæ…‹(å·¦åæ…‹)è³‡æ–™é›†ä¸­åœ¨å³é‚Š
      ![](https://i.imgur.com/slWRo8K.png)
      - [Kurtosis (å³°åº¦)](https://brewcode.stringlab.org/what-is-kurtosis-and-its-significance/) æ˜¯åœ¨è¡¡é‡è³‡æ–™çš„ tail æœ‰å¤šå¤§ï¼ŒExcess Kurtosis (è¶…å³°åº¦) å‰‡æ˜¯åœ¨è¡¡é‡å®ƒçš„ tail å’Œå¸¸æ…‹åˆ†å¸ƒçš„ tail æœ‰ç”šéº¼å·®
          -  è¶…å³°åº¦ç‚ºæ­£ç¨±ç‚º leptokurtic
          -  è¶…å³°åº¦ç‚ºè² ç¨±ç‚º platykurtic
          -  è¶…å³°åº¦ç‚º0å°±æ˜¯å¸¸æ…‹åˆ†å¸ƒï¼Œç¨±ç‚ºmesokurtic
![](https://i.imgur.com/KCEr2OC.jpg)

#### å¯ä»¥å¦‚ä½•é¸æ“‡è¦ç”¨å“ªå€‹çµ±è¨ˆæª¢é©—æ–¹æ³•?
[Normality Tests for Statistical Analysis: A Guide for Non-Statisticians](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/)
[ç„¡æ¯æ•¸çµ±è¨ˆ by AIA](http://www.hmwu.idv.tw/web/R_AI/v2/hmwu_StatR-05-1_NonParametric_basic.pdf)

è¦–è¦ºåŒ–æ–¹é¢
- è‹¥åªæœ‰ä¸€å€‹è®Šæ•¸å°±ç”¨ Q-Q plot
- æœ‰å¤šå€‹è®Šæ•¸å°±ç”¨ Boxplot
- è‹¥éœ€è¦å‘ˆç¾çµ¦éå°ˆæ¥­äººå“¡å°±å¯ä»¥ç•«å‡º histogram


çµ±è¨ˆæª¢é©—æ–¹é¢
- å„ªå…ˆä½¿ç”¨ Shapiro-Wilk test
- è‹¥æ˜¯åœ¨å°å¸¸æ…‹åˆ†å¸ƒä»¥å¤–çš„æ©Ÿç‡åˆ†å¸ƒåšæ¸¬è©¦çš„æ™‚å€™è¦æ”¹ç”¨ A-D test æˆ–æ˜¯ K-S test ï¼Œä¸èƒ½ç”¨ã€€Shapiro-Wilk test
- æœ€å¥½ä¸è¦ç”¨ K-S test

#### å¦‚æœä¸æ˜¯å¸¸æ…‹åˆ†å¸ƒçš„è™•ç†æ–¹æ³•
[HOW TO DEAL WITH NON-NORMALITY DATA](https://epicdatastudio.xyz/how-to-deal-with-non-normality-data/)

å¯èƒ½åŸå›  : 
- é›¢ç¾¤å€¼æˆ–æ¥µç«¯å€¼çš„å½±éŸ¿
- æ¬¡æ—ç¾¤
- è³‡æ–™é‘‘åˆ¥åŠ›ä¸è¶³
- è³‡æ–™æ”¶é›†å¾—ä¸å¤ 
- è§€å¯Ÿå€¼ç‚ºæ­£å€¼ä¸”è¶¨è¿‘æ–¼é›¶æˆ–æœ‰è‡ªç„¶æ¥µé™
- è³‡æ–™ç‚ºå…¶ä»–åˆ†å¸ƒ

æ–¹æ³• : 
1. æ”¹ç”¨ç„¡æ¯æ•¸æ–¹æ³•
2. è½‰æ›è³‡æ–™ 
   - ç§»é™¤æˆ–å–ä»£é›¢ç¾¤å€¼ : ä¿®å‰ª (Trimming) æˆ–æ˜¯ç¸®å°¾ (Winsorizing)
      - ç¸®å°¾æ˜¯æŒ‡è¨­å®šæŸå€‹ä¸åˆç†çš„ç¯„åœï¼Œé‡åˆ°é€™å€‹ç¯„åœçš„è³‡æ–™æ™‚å°±èªå®šç‚ºé›¢ç¾¤å€¼ï¼Œè€Œä¿®å‰ªæ˜¯ç›´æ¥æŠŠé€™äº›é›¢ç¾¤å€¼åˆªæ‰ 
   - è½‰æ›è³‡æ–™ : é–‹æ ¹è™Ÿã€å–å°æ•¸ã€å†ªæ¬¡è½‰æ› (power transformation )ã€éå»¶è½‰æ› (lagged transformation )
      - æœ€å¸¸ç”¨çš„æ˜¯ Box-Cox power transformations 

[Implementation of Box-Cox power transformations](https://www.geeksforgeeks.org/box-cox-transformation-using-python/)
```python=
import numpy as np
from scipy import stats
  
# plotting modules
import seaborn as sns
import matplotlib.pyplot as plt
  
# generate non-normal data (exponential)
original_data = np.random.exponential(size = 1000)
  
# transform training data & save lambda value
fitted_data, fitted_lambda = stats.boxcox(original_data)
  
# creating axes to draw plots
fig, ax = plt.subplots(1, 2)
  
# plotting the original data(non-normal) and 
# fitted data (normal)
sns.distplot(original_data, hist = False, kde = True,
            kde_kws = {'shade': True, 'linewidth': 2}, 
            label = "Non-Normal", color ="green", ax = ax[0])
  
sns.distplot(fitted_data, hist = False, kde = True,
            kde_kws = {'shade': True, 'linewidth': 2}, 
            label = "Normal", color ="green", ax = ax[1])
  
# adding legends to the subplots
plt.legend(loc = "upper right")
  
# rescaling the subplots
fig.set_figheight(5)
fig.set_figwidth(10)
  
print(f"Lambda value used for Transformation: {fitted_lambda}")
```

![](https://i.imgur.com/inPwtzx.png)

### çµ±è¨ˆè©¦é©— ( Statistical tests )

[Statistical Tests with Python
](https://python.plainenglish.io/statistical-tests-with-python-880251e9b572)

![](https://i.imgur.com/428Uhcr.jpg)


#### åˆ†ç‚ºæœ‰æ¯æ•¸è·Ÿç„¡æ¯æ•¸æ–¹æ³•
- æœ‰æ¯æ•¸ (parametric) : t æª¢å®šã€ANOVAã€Pearson coefficient of correlation
- ç„¡æ¯æ•¸ (nonparametric) : å¡æ–¹æª¢å®šã€The Sign Testã€The Median Testã€The Wilcoxon Rank Sum Testã€Mann-Whitney Testã€Kruskal-Wallis Testã€The Spearmanâ€™s Rank Test


#### ç„¡æ¯æ•¸æ–¹æ³•å„ªç¼ºåˆ†æ
å„ªé»ï¼š
- æ¯ç¾¤é«”åˆ†å¸ƒæœªçŸ¥æˆ–ä¸æ˜¯å¸¸æ…‹åˆ†å¸ƒï¼Œæˆ–æ˜¯æ¨£æœ¬æ•¸ä¸å¤ å¤§æ™‚çš†å¯ä½¿ç”¨ã€‚æ˜¯ç„¡æ¯æ•¸åˆ†ææ–¹æ³•çš„æœ€å¤§å„ªé»ã€‚
- è¨ˆç®—ç°¡å–®ä¸”å¿«é€Ÿã€‚
- é›–ç„¶åœ¨æ¯ç¾¤å¯¦éš›ä¸Šç‚ºå¸¸æ…‹åˆ†é…æ™‚ï¼Œè¼ƒæœ‰æ¯æ•¸åˆ†ææ–¹æ³•ä¸æ˜“å¾—åˆ°é¡¯è‘—çµæœï¼›ä½†åœ¨æ¯ç¾¤é«”ä¸æ˜¯å¸¸æ…‹åˆ†å¸ƒæ™‚ï¼Œç„¡æ¯æ•¸åˆ†ææ–¹æ³•ä¹‹æª¢åŠ›è¼ƒæœ‰æ¯æ•¸åˆ†ææ–¹æ³•é«˜ã€‚

ç¼ºé»ï¼š
- åªä½¿ç”¨è³‡æ–™çš„ç¬¦è™Ÿã€æ’åºç­‰ç‰¹æ€§ï¼Œæµªè²»äº†æ•¸å€¼ä¹‹é›†ä¸­è¶¨å‹¢ã€åˆ†æ•£æ€§åŠåˆ†ä½ˆæ‰€æä¾›çš„è³‡è¨Šã€‚
- é‡å°å¸¸æ…‹åˆ†å¸ƒè³‡æ–™å¦‚æœä»é€²è¡Œç„¡æ¯æ•¸åˆ†æï¼Œå°‡ä½¿æª¢åŠ›é™ä½ã€‚
- ç•¶æ¬²æª¢å®šçš„è³‡æ–™ä¸ç¬¦åˆæœ‰æ¯æ•¸åˆ†ææ³•ä¹‹å‡è¨­å‰ææ™‚æ‰å»ºè­°ä½¿ç”¨ç„¡æ¯æ•¸åˆ†ææ³•ï¼Œ
- ç‚ºä¸€ç¨®äº’è£œçš„çµ±è¨ˆæ–¹æ³•ï¼Œè€Œéç”¨æ–¼å–ä»£æœ‰æ¯æ•¸åˆ†ææ³•ã€‚


#### å¦‚ä½•é¸æ“‡çµ±è¨ˆè©¦é©—
![](https://i.imgur.com/U9SKZLR.png)
- [Overview](https://philipppro.github.io/Statistical_tests_overview/)
![](https://i.imgur.com/SvGFR0k.png)
![](https://i.imgur.com/a5g94AY.png)


#### Hypothesis Testing

[Hypothesis Testing Explained](https://www.kdnuggets.com/2021/09/hypothesis-testing-explained.html)

Type I and Type II error : 
   - è‹¥ $H_0$ ç‚º True ä½†å» Reject $H_0$ å°±å« Type I Errorï¼Œå…¶ç™¼ç”Ÿçš„æ©Ÿç‡ç­‰æ–¼ $\alpha$ï¼Œä¹Ÿå°±æ˜¯ level of significance (é¡¯è‘—æ°´æº–)
   - è‹¥ $H_a$ ç‚º True ä½†å» Accept $H_0$ å°±å« Type II Errorï¼Œå…¶ç™¼ç”Ÿçš„æ©Ÿç‡ç­‰æ–¼ $\beta$

![](https://i.imgur.com/G5L0Inc.jpg)

åªæ§åˆ¶ Type I Error çš„å‡è¨­æ‡‰ç”¨ç¨±ç‚º Significance tests (é¡¯è‘—æ€§æ¸¬è©¦)
- **P-Value** : å‡è¨­ Null hypothesis ($H_0$) æ˜¯æ­£ç¢ºçš„æƒ…æ³ä¸‹ï¼Œæ‰€è§€å¯Ÿåˆ°çš„çµ±è¨ˆé‡èˆ‡ç›®å‰å·²ç¶“å¯¦éš›è§€æ¸¬éçš„æ¨£æœ¬ä¸€æ¨£æˆ–æ˜¯æ›´åŠ æ¥µç«¯çš„æ©Ÿç‡
  - p-value åˆç¨± observed level of significance
  - åœ¨æŸå€‹æ©Ÿç‡åˆ†ä½ˆ(tåˆ†å¸ƒã€zåˆ†å¸ƒæˆ–æ˜¯å¸¸æ…‹åˆ†å¸ƒä¹‹é¡çš„)ä¸‹å¤§æ–¼è‡¨ç•Œå€¼çš„æ©Ÿç‡å¯†åº¦å€¼
  - p-value è¶Šå°è¡¨ç¤º $H_0$ è¶Šå¯èƒ½æ˜¯éŒ¯èª¤çš„ï¼Œä¹Ÿè¡¨ç¤ºæœ‰è¶Šå¤šæ¸¬é‡è­‰æ“šå­˜åœ¨æ”¯æŒ $H_a$ ç‚ºçœŸï¼Œé€™æ˜¯å› ç‚ºè§€å¯Ÿåˆ°äº†åœ¨ $H_0$ ç‚ºçœŸçš„å‡è¨­ä¸‹ ï¼Œå…¶ç›¸å°æ¥µç«¯ä¾‹å­çš„å‡ºç¾è€Œé€ æˆçš„

- **Critical Value** : æ¸¬è©¦æ¥å—å€åŸŸçš„é‚Šç•Œ(è‡¨ç•Œå€¼)ï¼Œé€™å€‹æ¸¬è©¦é€šå¸¸æ˜¯æŒ‡ null hypothesis
  - æ¦‚å¿µä¸Šå°±æ˜¯ level of significanceï¼Œé¡¯è‘—æ°´æº–ï¼Œåªæ˜¯å…¶å®šç¾©æœƒæ ¹æ“šæ‰€åšçš„æª¢å®šè€Œè®Šï¼Œæ¯”å¦‚åœ¨ t test ä¸­å°±æ˜¯ t å€¼ï¼Œåœ¨ z test ä¸­å°±æ˜¯ z å€¼
  - è‹¥è½‰æ›ç‚º p-value çš„è©±ï¼Œå…¶å¯¦å°±æ˜¯çµ±è¨ˆæª¢å®šçŠ¯éŒ¯(Type I error)çš„æ©Ÿç‡ï¼Œæ¯”å¦‚ $\alpha$=0.05ï¼Œå°±æ˜¯åªæœ‰ 5% çš„æ©Ÿç‡é€™å€‹æª¢å®šæœƒçŠ¯ Type I error ( èª¤å°‡$H0$ rejectæ‰çš„æ©Ÿç‡ )ï¼ŒçŠ¯éŒ¯çš„åŸå› æ˜¯éš¨æ©Ÿèª¤å·®
  - critical value of z æ˜¯æŒ‡å°‡ z åˆ†å¸ƒåˆ‡ç‚ºä¸­å¤®å€åŸŸå’Œå°¾ç«¯å€åŸŸçš„é‚£å€‹åˆ†å‰²é» ( z åˆ†å¸ƒå°±æ˜¯æ¨™æº–å¸¸æ…‹åˆ†å¸ƒ)

#### z æª¢å®š (z test) : $\sigma$ Known
ç‰¹æ€§ :
- å±¬æ–¼æœ‰æ¯æ•¸æ–¹æ³•
- é©ç”¨æ–¼ n>30ï¼Œ$\mu$ æœªçŸ¥ä½† $\sigma$ å·²çŸ¥çš„æƒ…æ³
- zåˆ†å¸ƒç­‰æ–¼æ¨™æº–å¸¸æ…‹åˆ†å¸ƒ ($\mu$=0ï¼Œ$Ïƒ^2$=1ä¹‹å¸¸æ…‹åˆ†å¸ƒ)

![](https://i.imgur.com/Wh4MnHx.jpg)

ç¨®é¡ : 
- One sample z-test
- Two independent sample z-test
- One sample z-test for proportion
- Two independent sample z-test for proportion
- Paired (Correlated) z-test : Match sample pair

#### t æª¢å®š (t test) : $\sigma$ Unknown

ç‰¹æ€§ :
- å±¬æ–¼æœ‰æ¯æ•¸æ–¹æ³•ï¼Œåˆç¨± student's t test
- é©ç”¨æ–¼ n<30, $\sigma$ æœªçŸ¥çš„æƒ…æ³
- åˆ©ç”¨ t å€¼ä¾†æ¨æ–·å‡ºå·®ç•°ç™¼ç”Ÿçš„æ©Ÿç‡ï¼Œç”¨å…¶æª¢æ¸¬å…©ç¾¤è³‡æ–™çš„å¹³å‡å€¼å·®ç•°æ˜¯å¦é¡¯è‘—
- ä»¥æ¨£æœ¬æ¨™æº–å·® $s$ ä¾†å–ä»£æœªçŸ¥çš„æ¨™æº–å·® $\sigma$
- è‡ªç”±åº¦ = n-1
  - ç•¶è‡ªç”±åº¦è¶Šå¤§ï¼Œt åˆ†å¸ƒè¶Šæ¥è¿‘å¸¸æ…‹åˆ†å¸ƒ
  - é€šå¸¸è‡ªç”±åº¦=30å°±æœƒè¦–ç‚º(è¿‘ä¼¼æ–¼)å¸¸æ…‹åˆ†ä½ˆ(zåˆ†å¸ƒ)

![](https://i.imgur.com/rCmeZ90.jpg)

ç¨®é¡ : 
- One sample t-test
- Two independent sample t-test
- Paired (Correlated) t-test : Match sample pair
- Pooled t-test (Equal Variance) : æ¯å€‹ group ä¸­çš„æ•¸é‡ä¸€æ¨£æˆ–æ˜¯å…©å€‹ dataset çš„ variance ç›¸è¿‘
![](https://i.imgur.com/SNQwtpA.png)
- [t åˆ†å¸ƒ å’Œ z åˆ†å¸ƒç•°åŒ](https://smallcollation.blogspot.com/2013/08/tzsimilarities-and-differences-between.html#gsc.tab=0)
![](https://i.imgur.com/oEbnnSb.jpg)


#### å¡æ–¹æª¢å®š (Pearson's Chi-squared test)

ç‰¹æ€§ : 
- å±¬æ–¼ç„¡æ¯æ•¸æ–¹æ³•ï¼Œç”¨ä¾†åˆ†æå…©è®Šæ•¸ä¹‹é–“çš„é—œä¿‚
- åªèƒ½ç”¨åœ¨é¡åˆ¥å‹è®Šæ•¸ï¼Œä¸”æ¨£æœ¬çš†ç‚ºç¨ç«‹çš„æƒ…æ³ä¸‹
  - å…¶ä»–çš„å‡è¨­ç‚º : 
    - è‡³å°‘æœ‰ 80% çš„ cell ä¸­å…¶æ¨£æœ¬æ•¸å¤§æ–¼5
    - æ¯ä¸€æª¢å®šç´°æ ¼(cell)å…§çš„æ•¸æ“šæ‡‰è©²è¨­ç‚ºé »ç‡æˆ–è¨ˆæ•¸æ•¸ç›®ï¼Œè€Œä¸æ˜¯ç™¾åˆ†æ¯”æˆ–æ˜¯ç¶“éè½‰æ›ä¹‹æ•¸æ“š
- Null hypothesis : ä¸€å€‹æ¨£æœ¬ä¸­å·²ç™¼ç”Ÿäº‹ä»¶çš„æ¬¡æ•¸åˆ†é…æœƒéµå®ˆæŸå€‹ç‰¹å®šçš„ç†è«–åˆ†é…
- åœ¨ Null hypothesis æˆç«‹æ™‚æœƒè¿‘ä¼¼æœå¾å¡æ–¹åˆ†å¸ƒçš„æª¢å®š 

ç¨®é¡ : 
- é¡¯è‘—æ€§æª¢å®š(Test of significance of change): æª¢å®šä¸€ç¾¤å—è©¦è€…å°äº‹ä»¶å‰å¾Œåæ‡‰çš„å·®ç•°
- é©åˆåº¦æª¢å®š(Test of goodness of fit): æª¢å®šè³‡æ–™æ˜¯å¦ç¬¦åˆæŸå€‹æ¯”ä¾‹é—œä¿‚æˆ–æŸå€‹æ©Ÿç‡åˆ†ä½ˆ
- åŒè³ªæ€§æª¢å®š(Test of homogeneity): åˆç¨±é½Šä¸€æ€§æª¢å®šï¼Œæª¢å®šå¹¾å€‹ä¸åŒé¡åˆ¥ä¸­çš„æ¯”ä¾‹é—œä¿‚æ˜¯å¦ä¸€è‡´
- ç¨ç«‹æ€§æª¢å®š(Test of independence): æª¢å®šå…©å€‹åˆ†é¡è®Šæ•¸ä¹‹é–“æ˜¯å¦äº’ç›¸ç¨ç«‹

å¡æ–¹åˆ†å¸ƒ(Chi-Squared Distribution)
  - **the chi-squared distribution is a particular case of the gamma distribution**
  - allows you to estimate confidence intervals for a population standard deviation
  - It is the distribution of sample variances when the underlying distribution is normal
  - You can test deviances of differences between expected and observed values
  - You can conduct a chi-squared test
![](https://i.imgur.com/6O57CQs.png)

### å…¶ä»–æ©Ÿç‡åˆ†å¸ƒ : Gamma, Geometric, Poisson, Exponential

- Gamma åˆ†å¸ƒ (ä¼½ç‘ªåˆ†å¸ƒ)
  - ç‚ºé€£çºŒå‹åˆ†å¸ƒ 
  - ç”¨ä¾†é æ¸¬æŸå€‹æœªä¾†äº‹ä»¶ç™¼ç”Ÿä¹‹å‰é ˆè¦ç­‰å¾…å¤šä¹…
  - ç”¨ä¾†é æ¸¬æŸäº›æœ€å°å€¼æœ¬ä¾†å°±æ˜¯0çš„äº‹ä»¶å¾ˆæœ‰ç”¨
  - ç‚ºå¡æ–¹åˆ†å¸ƒå’ŒæŒ‡æ•¸åˆ†å¸ƒçš„ generalized ç‰ˆæœ¬åˆ†å¸ƒ
![](https://i.imgur.com/KG3Xiln.png)

- Geometric åˆ†å¸ƒ (å¹¾ä½•åˆ†å¸ƒ)
  - ç‚ºé›¢æ•£å‹åˆ†å¸ƒ 
  - The geometric distribution represents the probability of having x Bernoulli(p) failures until first success?
    - ä¹Ÿå°±æ˜¯èªªå¹¾ä½•åˆ†å¸ƒå›ç­”äº† "ç›´åˆ°æˆåŠŸä¹‹å‰è¦å¤±æ•—å¤šå°‘æ¬¡?" é€™å€‹å•é¡Œ
  - ä¹Ÿèƒ½ç”¨å¹¾ä½•åˆ†å¸ƒä¾†æ‰¾åˆ° Bernoulli(1-p) å¾—åˆ°ä¸€æ¬¡æˆåŠŸæ‰€éœ€è¦çš„è©¦é©—æ¬¡æ•¸
  - è‹¥ä¸€å€‹äº‹ä»¶ç¬¦åˆå¹¾ä½•åˆ†å¸ƒï¼Œå¯ç”¨ä¾†æª¢æŸ¥ä¸€å€‹äº‹ä»¶æ˜¯å¦æ˜¯ i.i.d

- Poisson åˆ†å¸ƒ (åœç“¦æ¾åˆ†å¸ƒ)
  - ç‚ºé›¢æ•£å‹åˆ†å¸ƒ 
  - ç”¨ä¾†æè¿°å–®ä½æ™‚é–“å…§æŸå€‹éš¨æ©Ÿäº‹ä»¶ç™¼ç”Ÿçš„æ¬¡æ•¸
 ![](https://i.imgur.com/KmIuQ4t.png)

- Exponential åˆ†å¸ƒ (æŒ‡æ•¸åˆ†å¸ƒ)
  - ç‚ºé€£çºŒå‹åˆ†å¸ƒ 
  - ç”¨ä¾†è¡¨ç¤ºç¨ç«‹éš¨æ©Ÿäº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“é–“éš” 
  - èˆ‡åœç“¦æ¾åˆ†å¸ƒå¯†ä¸å¯åˆ†
    - å¦‚æœåœç“¦æ¾åˆ†é…é©åˆè¡¨ç¤ºæŸä¸€å€‹å€é–“å…§äº‹ä»¶ç™¼ç”Ÿæ¬¡æ•¸çš„æ©Ÿç‡ï¼ŒæŒ‡æ•¸åˆ†é…å°±å¯ä»¥æè¿°äºŒæ¬¡äº‹ä»¶ç™¼ç”Ÿçš„æ™‚é–“é–“éš”çš„æ©Ÿç‡ 
    - æ¯”å¦‚èªªï¼Œè‹¥ä¹˜å®¢æŠµé”äººæ•¸æ˜¯åœç“¦æ¾åˆ†å¸ƒï¼Œå‰‡ä¹˜å®¢ä¹‹é–“æŠµé”çš„æ™‚é–“é–“éš”å°±æœƒæ˜¯æŒ‡æ•¸åˆ†å¸ƒ ![](https://i.imgur.com/UTu2S3Y.png)

### é›¢ç¾¤å€¼æª¢æ¸¬ Outlier Check / Anomaly Detection
- ä¹Ÿç¨±ç•°å¸¸å€¼æª¢æ¸¬ (Anomaly dectection)

[Outlier Check for Dataset](https://datatest.readthedocs.io/en/stable/how-to/outliers.html#example-usage) 
[Multi-variate outlier detection in Python](https://towardsdatascience.com/multi-variate-outlier-detection-in-python-e900a338da10)
[5 Anomaly Detection Algorithms every Data Scientist should know](https://towardsdatascience.com/5-anomaly-detection-algorithms-every-


ç”¨åœ¨
1. è³‡æ–™å‰è™•ç†
2. ç¯©é¸ unlabeled dataï¼Œæ‰¾å‡ºå…¶ä¸­çš„ outlier
3. åˆ†é¡ labeded data çš„æ™‚å€™ï¼Œè‹¥é‡åˆ°é¡åˆ¥éå¸¸ä¸å¹³è¡¡çš„æƒ…æ³ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨ç•°å¸¸å€¼æª¢æ¸¬çš„æ¼”ç®—æ³•ä¾†åš

#### Tukey fence (interquartile Range, IQR, å››åˆ†ä½è·)
- æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œå¯é †ä¾¿ç•«å‡º Box plot
- æ˜¯åŸºæ–¼æ¯å€‹å€¼æœ¬èº«çš„ç‰¹æ€§å»æª¢æ¸¬ï¼Œä¸¦æ²’æœ‰è€ƒæ…®åˆ°è³‡æ–™ä¹‹é–“çš„ interaction
- $IQR = D3 - D1$
  - $QD = (Q3 - Q1) / 2$, å››åˆ†ä½å·®
- èƒ½ç”¨åœ¨ normal å’Œ  slightly skewed distribution
  - å…¶ä»–åµæ¸¬ outliner çš„æ–¹æ³•å› ç‚ºå°æ¥µç«¯å€¼éæ–¼æ•æ„Ÿéƒ½åªèƒ½ç”¨åœ¨ normal

æ­¥é©Ÿ: 
1. ç”¨å‡åº(ascending order)æ’åˆ—è³‡æ–™
   - values = sorted(values)  
2. æ‰¾åˆ°è³‡æ–™çš„ä¸­ä½æ•¸ 
   - midpoint = int(round(len(values) / 2.0))
3. Q1 ç‚ºç¬¬ä¸€å€‹è³‡æ–™åˆ°ä¸­ä½æ•¸é€™å€‹ç¯„åœçš„ä¸­ä½æ•¸ 
   - Q1 = median(values[:midpoint])
4. Q3 ç‚ºä¸­ä½æ•¸åˆ°æœ€å¾Œä¸€å€‹è³‡æ–™çš„ä¸­ä½æ•¸ 
   - Q3 = median(values[midpoint:])
5. ç®—å‡º IQR = Q3 - Q1
6. $Lower limit = Q1âˆ’(IQRÃ—multiplier)$
7. $Upper limit = Q3+(IQRÃ—multiplier)$
   - multiplier é€šå¸¸æ˜¯ç”¨ 1.5, è‹¥è¦åµæ¸¬æ›´é çš„å‰‡æ˜¯ç”¨ 3.0
   - ä¹Ÿæœ‰è«–æ–‡èªªç”¨ 2.2 è¼ƒç‚ºé€šç”¨
8. è‹¥æœ‰ä½æ–¼lower limit æˆ–æ˜¯é«˜æ–¼ Upper limit çš„å°±æ˜¯ Outlier
![](https://i.imgur.com/SB3z5Ax.png)

#### Isolation forest
[ioslation forest by sklearn](https://scikit-learn.org/stable/modules/outlier_detection.html#isolation-forest)

- åŸºæ–¼è³‡æ–™ä¹‹é–“çš„ interaction å»åˆ¤æ–· outlier
- å¸¸ç”¨åœ¨é€£çºŒå‹ã€é©åº¦çš„é«˜ç¶­è³‡æ–™ä»¥åŠå¤§é‡è³‡æ–™
  - é‡åˆ°éé«˜ç¶­åº¦çš„è³‡æ–™æœ€å¥½å…ˆç”¨ Kurtosis æˆ–æ˜¯å…¶ä»–æ–¹æ³•åšé™ç¶­ï¼Œä¸ç„¶æ•ˆèƒ½æœƒå¾ˆå·®
- å…ˆéš¨æ©Ÿé¸ä¸€å€‹ featureï¼Œå†å¾é€™å€‹ feature éš¨æ©Ÿé¸ä¸€å€‹ä»‹æ–¼æœ€å¤§å’Œæœ€å°å€¼ä¹‹é–“çš„ split valueï¼Œåˆ©ç”¨å®ƒä¾†å°è³‡æ–™åšäºŒåˆ†
  - å…¶å¯¦å°±æ˜¯åœ¨å»ºæ§‹ä¸€å€‹éš¨æ©Ÿå‹çš„æ±ºç­–æ¨¹
  - ç¨ç«‹å‡ºä¸€å€‹ sample æ‰€éœ€è¦çš„ split value æ•¸æœƒæ­£å¥½ç­‰æ–¼é€™å€‹æ±ºç­–æ¨¹å¾ Root èµ°åˆ° leaf node æ‰€éœ€æ­¥æ•¸ (path length)
  - é›¢ Root è¶Šè¿‘çš„è¶Šå¯èƒ½æ˜¯ outlier
- é€šå¸¸æœƒåšå¾ˆå¤šæ¬¡å†æŠ½æ¨£ä»¥åŠæœƒå°è³‡æ–™åšæŠ½æ¨£ï¼Œä»¥è¼ƒå°‘é‡çš„è³‡æ–™ä¾†è¨“ç·´å³å¯
  - å°è³‡æ–™æŠ½æ¨£å¯é¿å… swampingï¼Œä¹Ÿå°±æ˜¯æ­£å¸¸è³‡æ–™å’Œç•°å¸¸è³‡æ–™éæ–¼æ¥è¿‘è€Œç„¡æ³•åˆ†é›¢ï¼Œé€šå¸¸åœ¨è³‡æ–™éå¤šçš„æ™‚å€™ç™¼ç”Ÿ
  - è³‡æ–™æŠ½æ¨£ä¹Ÿå¯é¿å… Msakingï¼Œä¹Ÿå°±æ˜¯ç•¶ç•°å¸¸å€¼éå¤šä¸”åˆè¢«åˆ†åœ¨åŒä¸€å€‹ cluster ä¸­çš„æ™‚å€™æœƒå¾ˆé›£åˆ†é–‹å–®ä¸€å€‹ç•°å¸¸å€¼
- æœ‰è¶£çš„æ˜¯ä¹Ÿå¯ä»¥åšå–®æ¨£æœ¬ï¼Œä¹Ÿå°±æ˜¯åªæœ‰æ­£å¸¸çš„æ¨£æœ¬ä½†æ²’æœ‰ç•°å¸¸æ¨£æœ¬çš„è¨“ç·´æƒ…æ³
  - å¦å¤–ä¸€å€‹å¯ä»¥åšå–®æ¨£æœ¬çš„æ˜¯ **One-Class SVM**ï¼ŒOne-Class SVMæœƒæ›´é©åˆç”¨åœ¨ä¸­å°å‹çš„è³‡æ–™ä¸Š

ä¾‹å­ : æ¯”å¦‚å° a,b,c,d åš iForest

![](https://i.imgur.com/nmvJ5ju.png)
- å…¶ä¸­çš„ d æœ€æœ‰å¯èƒ½æ˜¯ç•°å¸¸å€¼

#### Local outlier factor (LOF)
[sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html#sklearn.neighbors.LocalOutlierFactor)
- è·ŸiForestä¸€æ¨£é©åˆç”¨åœ¨é©åº¦é«˜ç¶­çš„è³‡æ–™
- ç‰¹é»åœ¨æ–¼å®ƒåŒæ™‚è€ƒæ…®äº†å±€éƒ¨å’Œå…¨å±€çš„é—œä¿‚ä¾†æ‰¾å‡º outlier
- å°æ¯ä¸€å€‹è³‡æ–™é»åšè©•åˆ†ï¼Œé€™å€‹åˆ†æ•¸å«åš local outlier factorï¼Œå…¶ä»£è¡¨é€™å€‹è³‡æ–™é»çš„ç•°å¸¸åº¦ (the degree of abnormality)
  - LOF æ˜¯æ ¹æ“šè³‡æ–™é»æœ¬èº«ç›¸å°æ–¼å…¶é„°å±…çš„å±€éƒ¨å¯†åº¦(local density)
  - ç•¶è³‡æ–™é»çš„å¯†åº¦æ¯”é„°å±…ä½è¶Šå¤šï¼Œå‰‡è¶Šæœ‰å¯èƒ½æ˜¯ outlierï¼Œå› ç‚ºé€™è¡¨ç¤ºå®ƒçš„é„°å±…è¼ƒå°‘ï¼Œé€™æ™‚å€™é€™å€‹è³‡æ–™é»çš„ LOF æœƒ >> 1
- å¯¦ä½œä¸Šé€™å€‹ Local density æ˜¯é€éè³‡æ–™é»å’Œå®ƒé„°å±…ä¹‹é–“çš„è·é›¢ä¾†è¡¡é‡çš„
  - å…¶å¯¦å°±æ˜¯å…ˆç®—å‡º k-nearest neighborsï¼Œç„¶å¾Œä¾æ“šå…¶çµæœä¾†ç®—å‡ºæ¯å€‹é»çš„ LOF
  - outlier ä½”è³‡æ–™é›†çš„æ¯”ä¾‹è¶Šé«˜ï¼Œkçš„å€¼å°±æœƒéœ€è¦è¶Šå¤§

åˆ¤æ–·æ¨™æº–
- LOF ~ 1  =>  Similar data point
- LOF < 1  =>  Inlier ( similar data point which is - inside the density cluster)
- LOF > 1  =>  Outlier

![](https://i.imgur.com/GWEO7AJ.jpg)


### Feature Scaling ç‰¹å¾µç¸®æ”¾

[Data Transformation: Standardization vs Normalization](https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html)

- The goal of applying Feature Scaling is to **make sure features are on almost the same scale** so that each feature is equally important and make it easier to process by most ML algorithms
- æœ‰ Standardization (æ¨™æº–åŒ–) å’Œ Min-Max Normalization (æ­¸ä¸€åŒ–) å…©ç¨®æ–¹æ³•

éœ€è¦åš feature scaling çš„æ¼”ç®—æ³•
![](https://i.imgur.com/SQ7xYz7.png)
- å°æ–¼ä¸éœ€è¦è·é›¢çš„æ¼”ç®—æ³•ï¼Œfeature scaling ä¸é‡è¦
  -  åƒæ˜¯ Naive Bayes, Linear Discriminant Analysis, and Tree-Based models (gradient boosting, random forest 

#### Standardization (Z-score normalization)
- ä¸­å¿ƒæ¨™æº–åŒ–æ–¹æ³•
- æŠŠè³‡æ–™éƒ½ scale æˆ mean=0, std=1
- å¯ä»¥æ›´å¥½çš„è§£æ±º Outlier å•é¡Œï¼Œå› æ­¤é€šå¸¸æœƒä½¿ç”¨é€™å€‹æ–¹æ³•
![](https://i.imgur.com/r3oAg9w.png)

#### Min-Max Normalization 
- é›¢å·®æ¨™æº–åŒ–æ–¹æ³•
- æŠŠè³‡æ–™éƒ½ scale åˆ°ä¸€å€‹æœ€å¤§å’Œæœ€å°å€¼çš„å€é–“å€¼ (Ex: Scale åˆ°0~1)
- ä¸èƒ½è§£æ±º Outlier å•é¡Œ
![](https://i.imgur.com/DWvzZyc.png)


#### MaxAbsScaler
- èˆ‡ MinMaxScaler é¡ä¼¼
- æ‰€æœ‰æ•¸æ“šéƒ½æœƒé™¤ä»¥è©²åˆ—çµ•å°å€¼å¾Œçš„æœ€å¤§å€¼
- æ•¸æ“šæœƒç¸®æ”¾åˆ°åˆ°[-1,1]ä¹‹é–“
  - åˆ†æ¯çš„æœ€å¤§å€¼çµ•å°å€¼çš„åŸå§‹è³‡æ–™åœ¨è½‰æ›å¾Œä¸€å®šæœƒè®Šæˆ-1æˆ–æ˜¯1
- å¯ä»¥ç”¨åœ¨ CSR æˆ–æ˜¯ CSC ç¨€ç–çŸ©é™£æˆ–æ˜¯zero-center data
  - èƒ½å¤ ä¿ç•™ç¨€ç–æ€§ 
$$x_i = \frac{x_i}{max(\lvert x_i\rvert)}$$

#### Robust scalar
- æ¨æ£„æ‰ä¸­ä½æ•¸ä¸¦é™¤ä»¥ IQR ä¾†ç¸®æ”¾
- å¯ä»¥æœ‰æ•ˆçš„ç¸®æ”¾å¸¶æœ‰ outlier çš„æ•¸æ“š
  -  å¦‚æœæ•¸æ“šä¸­å«æœ‰ç•°å¸¸å€¼ï¼Œåœ¨ç¸®æ”¾éç¨‹ä¸­æœƒè¢«æ¨å»
$$x_{robust} = \frac{x - median(x)}{IQR}$$
$$IQR = Q3 - Q1$$


### Metrics
[Metrics to judge the sucess of a model](http://gael-varoquaux.info/interpreting_ml_tuto/content/01_how_well/01_metrics.html#classification-settings)

[Anomaly Detection â€” How to Tell Good Performance from Bad](https://towardsdatascience.com/anomaly-detection-how-to-tell-good-performance-from-bad-b57116d71a10)

#### Classification Metrics

Confusion Matrix æ··æ·†çŸ©é™£ç›¸é—œ
- **Sensitivity, Recall, TPR, å¬å›ç‡)** = TP / P = TP / (TP+FN)
   - åˆ¤æ–·æœ‰ç—…è€…ç‚ºé™½æ€§ä¹‹æ¯”ç‡
- **Specificity, selectivity, TNR** = TN / N = TN / (TN+FP)
   - åˆ¤æ–·ç„¡ç—…è€…ç‚ºé™°æ€§ä¹‹æ¯”ç‡
- **False Positive Rate (FPR)** = FP / N = FP / (FP+TN) = **1 - Specificity**
   - Type I Error, åˆ¤æ–·ç„¡ç—…è€…ç‚ºæœ‰ç—…ä¹‹æ¯”ç‡
- **Miss rate, False Negative Rate (FNR)** = FN / P = FN / (FN + TP) = **1 - Sensitivity** 
   - Type II Error, åˆ¤æ–·æœ‰ç—…è€…ç‚ºç„¡ç—…ä¹‹æ¯”ç‡


```python=
# Confusion Matrix in Python with Pandas
import pandas as pd
y_actu = pd.Series([2, 0, 2, 2, 0, 1, 1, 2, 2, 0, 1, 2], name='Actual')
y_pred = pd.Series([0, 0, 2, 1, 0, 2, 1, 0, 2, 0, 2, 2], name='Predicted')
df_confusion = pd.crosstab(y_actu, y_pred)
print (df_confusion)

Output:
Predicted  0  1  2
Actual
0          3  0  0
1          0  1  2
2          2  1  3
```



![](https://i.imgur.com/sdrh7zk.png)

æœ‰äº›æ‡‰ç”¨åœ¨æ„çš„æ˜¯ Precisionï¼Œæœ‰äº›æ‡‰ç”¨åœ¨æ„çš„æ˜¯ Recall
- å¦‚æœæ˜¯é–€ç¦ç³»çµ±ï¼Œæˆ‘å€‘å¸Œæœ›Precisionå¯ä»¥å¾ˆé«˜ï¼ŒRecallå°±ç›¸è¼ƒæ¯”è¼ƒä¸é‡è¦ï¼Œæˆ‘å€‘æ¯”è¼ƒåœ¨æ„çš„æ˜¯é æ¸¬æ­£å‘ï¼ˆé–‹é–€ï¼‰çš„ç­”å°å¤šå°‘ï¼Œæ¯”è¼ƒä¸åœ¨æ„å¯¦éš›æ­£å‘ï¼ˆæ˜¯ä¸»äººï¼‰çš„ç­”å°å¤šå°‘
   - Precision æ˜¯æŒ‡åˆ†é¡æ­£ç¢ºçš„æ­£æ¨£æœ¬å åˆ†é¡å™¨åˆ¤å®šç‚ºæ­£æ¨£æœ¬çš„æ¨£æœ¬å€‹æ•¸çš„æ¯”ä¾‹ 
   - Precision é«˜è€Œ Recall ä½çš„æ¨¡å‹æ˜¯ä¸€å€‹éå¸¸è¬¹æ…çš„æ¨¡å‹
- å¦‚æœæ˜¯å»£å‘ŠæŠ•æ”¾ï¼Œå‰‡Recallå¾ˆé‡è¦ï¼ŒPrecisionå°±é¡¯å¾—æ²’é€™éº¼é‡è¦äº†ï¼Œå› ç‚ºæ­¤æ™‚æˆ‘å€‘æ¯”è¼ƒåœ¨æ„çš„æ˜¯å¯¦éš›æ­£å‘ï¼ˆæ˜¯æ½›åœ¨å®¢æˆ¶ï¼‰çš„ç­”å°å¤šå°‘ï¼Œè€Œç›¸å°æ¯”è¼ƒä¸åœ¨æ„é æ¸¬æ­£å‘ï¼ˆå»£å‘ŠæŠ•å‡ºï¼‰ç­”å°å¤šå°‘ã€‚
   - Recall æ˜¯æŒ‡åˆ†é¡æ­£ç¢ºçš„æ­£æ¨£æœ¬å çœŸæ­£çš„æ­£æ¨£æœ¬å€‹æ•¸çš„æ¯”ä¾‹
   - Recall é«˜è€Œ Precison ä½çš„æ¨¡å‹æ˜¯ä¸€å€‹å¯¬é¬†çš„æ¨¡å‹

å››å€‹åŸºæœ¬æŒ‡æ¨™ä»¥å¤–çš„å…¶ä»–ç›¸é—œ metrics
- **Accuracy, æº–ç¢ºç‡** = (TP + TN) / (P + N) = (TP + TN) / (TP + TN + FP + FN)
  - é æ¸¬æ­£ç¢ºçš„æ¯”ç‡
- **balanced accuracy** = (TPR + TNR) / 2
  - ç”¨åœ¨ä¸å¹³è¡¡è³‡æ–™é›†
- **Precision, Positive Predictive Value (PPV), æº–ç¢ºç‡** = TP / (TP+FP)
  - æœ‰ç—…è€…å é™½æ€§ä¹‹æ¯”ç‡
- **Negative Predictive Value (NPV)** = TN / (TN + FN)
  - ç„¡ç—…è€…å é™°æ€§ä¹‹æ¯”ç‡
- **F1-score** = 2 * Precision * Recall / (Precision + Recall)

#### Regression Metrics

MAE, MSE, RMSE, R-squared, MAPE

- MAE è¼ƒèƒ½è™•ç† outlierï¼ŒMSE é‡åˆ° outlier å‰‡æœƒæŒ‡æ•¸æ”¾å¤§å®ƒ
- RMSE æ˜¯ç‚ºäº†é¿å…é‡åˆ°å¹³æ–¹å‡ºä¾†æœƒå¾ˆå¤§çš„è³‡æ–™è€Œåšçš„è™•ç†
- ${\displaystyle {\mbox{MAPE}}={\frac {100}{n}}\sum _{t=1}^{n}\left|{\frac {A_{t}-F_{t}}{A_{t}}}\right|}$
  - $A_t$æ˜¯å¯¦éš›å€¼
  - $F_t$æ˜¯é æ¸¬å€¼
  - è‹¥æœ‰è³‡æ–™æœƒç­‰æ–¼0ä¸å¯ç”¨
  - ä¸»è¦æ˜¯ç”¨ä¾†æ¯”è¼ƒæŸå…©è€…ï¼Œè€ƒæ…®çš„æ˜¯ç›¸å°èª¤å·®
  - æ¯”å¦‚å…©é–“é£²æ–™åº—è³£çš„é£²æ–™æ•¸ï¼Œé€™æ¨£æ‰å¯ä»¥æŠŠçœŸæ­£çš„å¯¦éš›å€¼ä¹Ÿè€ƒæ…®é€²å»ï¼Œæ‰ä¸æœƒæŠŠé æ¸¬99æ¯ã€è³£å‡º98æ¯èˆ‡é æ¸¬99999æ¯ã€è³£å‡º99998æ¯çš„é£²æ–™åº—ç•¶æˆä¸€æ¨£
- R-Squared = 1 - SSR / SST
- Adj R-Squared å‰‡æ˜¯æŠŠè‡ªè®Šæ•¸çš„æ•¸é‡ä¹Ÿè€ƒæ…®é€²å»

Tips

- é¸ MAE: æœ‰ç•°å¸¸å€¼çš„æƒ…æ³ï¼Œå¦‚æœä¸æƒ³è¦é€™äº›ç•°å¸¸å€¼å½±éŸ¿æ¨¡å‹
- é¸ MSE: æœ‰ä¸€é»ç•°å¸¸å€¼çš„æƒ…æ³ï¼Œå¦‚æœæƒ³è¦åŒ…å«é€™äº›ç•°å¸¸å€¼
- å¦‚æœä½¿ç”¨ MAE ä½œç‚ºæ€§èƒ½è©•ä¼°æŒ‡æ¨™å¾—åˆ°å¾ˆå¤§åå·®ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨ RMSE ä¾†æ’«å¹³å…¶åå·®
    - ä½†è‹¥é›¢ç¾¤å€¼çš„å€‹åˆ¥åå·®ç¨‹åº¦éå¸¸å¤§ï¼ŒRMSEé‚„æ˜¯æœƒè¡¨ç¾å¾—å¾ˆå·®ï¼Œæ­¤æ™‚å¾—æ”¹ç”¨ MAPE
- å¦‚æœè³‡æ–™é›†åŒ…å«å¾ˆå¤šç•°å¸¸å€¼ï¼Œå°è‡´é æ¸¬çµæœç”¢ç”Ÿåç§»ï¼Œä½ å¯èƒ½éœ€è¦ç”¨ MAE
- é¢å°å¯¦éš›å€¼è¼ƒä½çš„åºåˆ—ï¼Œå¯ä»¥å°‡å…¶èšåˆåˆ°ä¸€å€‹æ›´å¤§çš„æ™‚é–“ç¯„åœã€‚ä¾‹å¦‚ï¼Œå¦‚æœä»¥æ˜ŸæœŸç‚ºå‘¨æœŸçš„å€¼å¾ˆä½ï¼Œä½ å¯ä»¥è©¦è©¦æŒ‰ç…§æœˆä»½ä¾†é€²è¡Œé æ¸¬ï¼Œç”šè‡³æŒ‰å­£åº¦é æ¸¬ã€‚ä½ ä¹Ÿå¯ä»¥é€šéç°¡å–®çš„é™¤æ³•ï¼ŒæŠŠåŸå§‹æ™‚é–“åºåˆ—åˆ†è§£åˆ°è¼ƒå°çš„æ™‚é–“ç¯„åœä¸Šã€‚é€™ä¸€æ–¹æ³•å¯ä»¥å¹«åŠ©ä½ æ›´å¥½åœ°ä½¿ç”¨ MAE ä½œç‚ºè©•ä¼°æŒ‡æ¨™ï¼ŒåŒæ™‚å°å³°å€¼åšå¹³æ»‘è™•ç†ã€‚

#### Ranking Metrics

$\text{AP} = \sum_n (R_n - R_{n-1}) P_n$
  - where $R_n$ and $P_n$ are the precision and recall at the $n_{th}$ threshold,
  - MAP is the mean of AP over all the queries

#### Similarity Metrics
- $Cosine(x,y) = \frac{x \cdot y}{|x||y|}$
- $Jaccard(U,V) = \frac{|U \cap V|}{|U \cup V|}$
  - Similarity of two sets $U$ and $V$ .
- $PMI(x;y) = \log{\frac{p(x,y)}{p(x)p(y)}}$
  - Pointwise Mutual Information(PMI)
  - Relevance of two events $x$ and $y$

## Take-home Exercise

[How To Ace The ML Engineer Take-Home Interview Exercise](https://towardsdatascience.com/how-to-ace-the-ml-engineer-take-home-interview-exercise-daf5ba590de4)